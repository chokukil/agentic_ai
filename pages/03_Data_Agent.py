import streamlit as st
import glob
import json
import os
from pathlib import Path
from copy import deepcopy
from dotenv import load_dotenv, find_dotenv
import asyncio
import nest_asyncio
import platform
from langgraph.prebuilt import create_react_agent
from langchain_core.messages import HumanMessage, AIMessage
from langchain_mcp_adapters.client import MultiServerMCPClient
from utils import (
    astream_graph, random_uuid, generate_followups, get_followup_llm, 
    PandasAgentStreamParser, AgentCallbacks, pandas_tool_callback, 
    pandas_observation_callback, pandas_result_callback,
    # ì¶”ê°€ import - pandas agent ì‹¤ì‹œê°„ ì²˜ë¦¬ìš©
    tool_callback, observation_callback, result_callback,
    AgentStreamParser, ToolChunkHandler, display_message_tree,
    pretty_print_messages, messages_to_history, get_role_from_messages
)
from langchain_core.messages.ai import AIMessageChunk
from langchain_core.messages.tool import ToolMessage
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig
import requests
import sys
from contextlib import asynccontextmanager
from mcp.client import stdio as _stdio
import logging
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import BaseMessage
from langchain_core.outputs import ChatResult, ChatGeneration
from pydantic import ConfigDict
import yaml
from langchain_openai import ChatOpenAI
import pandas as pd
from langchain_experimental.agents.agent_toolkits.pandas.base import create_pandas_dataframe_agent
from urllib.parse import urlsplit, parse_qs
from utils import generate_followups, get_followup_llm
from langchain_experimental.tools import PythonAstREPLTool
from langchain.agents.agent_types import AgentType
import time

# ğŸ†• ë°ì´í„° ë¶„ì„ íŒ¨í‚¤ì§€ë“¤ import
import numpy as np
import scipy
import seaborn as sns
import sklearn
from sklearn import datasets, metrics, model_selection, preprocessing, linear_model, ensemble, cluster
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# ğŸ”§ ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ imports ì¶”ê°€
from langchain.agents import AgentExecutor
from langchain.schema.runnable import RunnablePassthrough
from langchain_core.runnables import RunnableLambda
from langchain_core.tools import Tool
from typing import Dict, Any, List, Optional
import functools
import threading

# ğŸ”§ MCP ë„êµ¬ë¥¼ pandas agentì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë™ê¸° í˜¸í™˜ìœ¼ë¡œ ë§Œë“œëŠ” í•¨ìˆ˜ (ê°œì„ )
def make_mcp_tool_sync_compatible(mcp_tool):
    """
    MCP ë„êµ¬ë¥¼ pandas agentì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë™ê¸° í˜¸í™˜ì„±ì„ ì¶”ê°€
    pandas agentëŠ” ë™ê¸°ì ìœ¼ë¡œ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ ë¹„ë™ê¸° MCP ë„êµ¬ë“¤ì„ ë˜í•‘í•´ì•¼ í•¨
    """
    try:
        # nest_asyncioê°€ ì´ë¯¸ ì ìš©ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        nest_asyncio.apply()
        
        def sync_tool_runner(*args, **kwargs):
            """ë™ê¸° ë„êµ¬ ì‹¤í–‰ ë˜í¼ (pandas agentìš©) - ê°œì„ ëœ ë²„ì „"""
            try:
                # ì¸ìê°€ dict í˜•íƒœë¡œ ì˜¤ëŠ” ê²½ìš° ì²˜ë¦¬
                if len(args) == 1 and isinstance(args[0], dict):
                    tool_input = args[0]
                else:
                    tool_input = kwargs if kwargs else {}
                
                logging.debug(f"Sync tool runner called for {mcp_tool.name} with input: {tool_input}")
                
                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
                try:
                    loop = asyncio.get_running_loop()
                    logging.debug("Found running event loop, using thread pool")
                    
                    # ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
                    result_container = []
                    exception_container = []
                    
                    def run_in_new_thread():
                        try:
                            new_loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(new_loop)
                            try:
                                if hasattr(mcp_tool, 'ainvoke'):
                                    result = new_loop.run_until_complete(mcp_tool.ainvoke(tool_input))
                                elif hasattr(mcp_tool, 'invoke'):
                                    result = mcp_tool.invoke(tool_input)
                                else:
                                    # í•¨ìˆ˜ í˜¸ì¶œ ë°©ì‹ ì‹œë„
                                    result = new_loop.run_until_complete(mcp_tool(tool_input))
                                result_container.append(result)
                            finally:
                                new_loop.close()
                        except Exception as e:
                            logging.error(f"Thread execution error for {mcp_tool.name}: {e}")
                            exception_container.append(e)
                    
                    thread = threading.Thread(target=run_in_new_thread)
                    thread.start()
                    thread.join(timeout=60)  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ
                    
                    if exception_container:
                        raise exception_container[0]
                    elif result_container:
                        logging.debug(f"Sync tool runner success for {mcp_tool.name}: {str(result_container[0])[:100]}")
                        return result_container[0]
                    else:
                        raise TimeoutError(f"Tool {mcp_tool.name} execution timed out")
                        
                except RuntimeError:
                    # ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ì§ì ‘ ì‹¤í–‰
                    logging.debug("No running event loop, executing directly")
                    if hasattr(mcp_tool, 'ainvoke'):
                        result = asyncio.run(mcp_tool.ainvoke(tool_input))
                    elif hasattr(mcp_tool, 'invoke'):
                        result = mcp_tool.invoke(tool_input)
                    else:
                        result = asyncio.run(mcp_tool(tool_input))
                    
                    logging.debug(f"Direct execution success for {mcp_tool.name}: {str(result)[:100]}")
                    return result
                        
            except Exception as e:
                error_msg = f"Tool execution error for {mcp_tool.name}: {str(e)}"
                logging.error(error_msg)
                return error_msg
        
        # ìƒˆë¡œìš´ ë™ê¸° í˜¸í™˜ ë„êµ¬ ìƒì„±
        compatible_tool = Tool.from_function(
            sync_tool_runner,
            name=mcp_tool.name,
            description=mcp_tool.description or "",
            infer_schema=True,
        )
        compatible_tool.handle_tool_error = True

        
        # ì›ë³¸ ì†ì„±ë“¤ ë³µì‚¬
        if hasattr(mcp_tool, 'args_schema'):
            compatible_tool.args_schema = mcp_tool.args_schema
        if hasattr(mcp_tool, 'return_direct'):
            compatible_tool.return_direct = mcp_tool.return_direct
            
        logging.debug(f"Made MCP tool {mcp_tool.name} sync compatible")
        return compatible_tool
        
    except Exception as e:
        logging.error(f"Failed to make MCP tool sync compatible: {e}")
        return mcp_tool  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜

class AsyncPandasAgentWrapper:
    """
    pandas agentë¥¼ ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ ë˜í•‘í•˜ëŠ” í´ë˜ìŠ¤ (ê°œì„ ëœ ë²„ì „)
    """
    def __init__(self, pandas_agent):
        self.pandas_agent = pandas_agent
        self.executor = None
        
    async def astream(self, inputs: Dict[str, Any], config: Optional[RunnableConfig] = None):
        """
        pandas agentë¥¼ ë¹„ë™ê¸°ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ - ê°œì„ ëœ ë²„ì „
        """
        try:
            import asyncio
            import concurrent.futures
            
            if self.executor is None:
                self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)
            
            # pandas agentì˜ ìŠ¤íŠ¸ë¦¼ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            def run_pandas_stream():
                try:
                    logging.debug("Starting pandas agent stream execution")
                    steps = []
                    
                    # pandas agent stream ì‹¤í–‰
                    for step in self.pandas_agent.stream(inputs, config=config):
                        logging.debug(f"Pandas agent step: {type(step)} - {step}")
                        steps.append(step)
                        
                    logging.debug(f"Pandas agent completed with {len(steps)} steps")
                    return steps
                    
                except Exception as e:
                    logging.error(f"Pandas agent stream error: {e}")
                    import traceback
                    logging.error(f"Traceback: {traceback.format_exc()}")
                    return [{"error": str(e)}]
            
            # ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            steps = await loop.run_in_executor(self.executor, run_pandas_stream)
            
            # ê²°ê³¼ë¥¼ í•˜ë‚˜ì”© yield
            for step in steps:
                yield step
                # ì‘ì€ ì§€ì—°ì„ ì¶”ê°€í•˜ì—¬ ì‹¤ì‹œê°„ ëŠë‚Œ ì œê³µ
                await asyncio.sleep(0.01)
                
        except Exception as e:
            logging.error(f"AsyncPandasAgentWrapper.astream error: {e}")
            yield {"error": str(e)}
    
    async def ainvoke(self, inputs: Dict[str, Any], config: Optional[RunnableConfig] = None, **kwargs):
        """
        pandas agentë¥¼ ë¹„ë™ê¸°ë¡œ ë‹¨ì¼ ì‹¤í–‰ - ê°œì„ ëœ ë²„ì „
        """
        try:
            import asyncio
            import concurrent.futures
            
            if self.executor is None:
                self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)
            
            def run_pandas_invoke():
                try:
                    logging.debug("Starting pandas agent invoke execution")
                    result = self.pandas_agent.invoke(inputs, config=config)
                    logging.debug(f"Pandas agent invoke result: {type(result)} - {str(result)[:200]}")
                    return result
                except Exception as e:
                    logging.error(f"Pandas agent invoke error: {e}")
                    import traceback
                    logging.error(f"Traceback: {traceback.format_exc()}")
                    return {"error": str(e)}
            
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(self.executor, run_pandas_invoke)
            return result
            
        except Exception as e:
            logging.error(f"AsyncPandasAgentWrapper.ainvoke error: {e}")
            return {"error": str(e)}

# Base directory for app icons
ASSETS_DIR = "assets"
URL_BASE = "http://localhost:2025/Agent?id="

# Configure logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# Log session state initialization
logging.debug('Initializing session state')

if platform.system() == "Windows":
    logging.debug(f"Using proactor: IocpProactor")
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

# nest_asyncio ì ìš©: ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ ë‚´ì—ì„œ ì¤‘ì²© í˜¸ì¶œ í—ˆìš©
nest_asyncio.apply()

# ì „ì—­ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„± ë° ì¬ì‚¬ìš© (í•œë²ˆ ìƒì„±í•œ í›„ ê³„ì† ì‚¬ìš©)
if "event_loop" not in st.session_state:
    loop = asyncio.new_event_loop()
    st.session_state.event_loop = loop
    asyncio.set_event_loop(loop)


st.set_page_config(
    page_title="AI Agent Builder",
    layout="wide",
    initial_sidebar_state="expanded",
)


OUTPUT_TOKEN_INFO = {
    "o4-mini": {"max_tokens": 16000},
    "gpt-4o": {"max_tokens": 16000},
}

# ğŸ†• ë°ì´í„° ë¶„ì„ í™˜ê²½ ì„¤ì • í•¨ìˆ˜
def create_data_analysis_environment(df=None):
    """
    ë°ì´í„° ë¶„ì„ì— í•„ìš”í•œ ëª¨ë“  íŒ¨í‚¤ì§€ë“¤ì„ ì‚¬ì „ì— ë¡œë“œí•œ í™˜ê²½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    plt.show()ë¥¼ ìë™ìœ¼ë¡œ Streamlit í˜¸í™˜ ë²„ì „ìœ¼ë¡œ íŒ¨ì¹˜í•©ë‹ˆë‹¤.
    
    Args:
        df: ë¶„ì„í•  DataFrame (ì„ íƒì‚¬í•­)
    
    Returns:
        dict: ì‚¬ì „ ë¡œë“œëœ íŒ¨í‚¤ì§€ë“¤ê³¼ ë°ì´í„°ë¥¼ í¬í•¨í•œ í™˜ê²½ ë”•ì…”ë„ˆë¦¬
    """
    # ğŸ†• í•œê¸€ í°íŠ¸ ì„¤ì • ì¶”ê°€
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm
    import platform
    import warnings
    
    def setup_korean_font():
        """í•œê¸€ í°íŠ¸ë¥¼ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜"""
        try:
            # Windows í™˜ê²½ì—ì„œ í•œê¸€ í°íŠ¸ ì„¤ì •
            if platform.system() == 'Windows':
                # Windowsì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸ë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
                korean_fonts = ['Malgun Gothic', 'Arial Unicode MS', 'Gulim', 'Dotum', 'Batang']
                
                for font_name in korean_fonts:
                    try:
                        # í°íŠ¸ê°€ ì‹œìŠ¤í…œì— ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                        available_fonts = [f.name for f in fm.fontManager.ttflist]
                        if font_name in available_fonts:
                            plt.rcParams['font.family'] = font_name
                            plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
                            
                            # í°íŠ¸ í…ŒìŠ¤íŠ¸
                            fig, ax = plt.subplots(figsize=(1, 1))
                            ax.text(0.5, 0.5, 'í•œê¸€í…ŒìŠ¤íŠ¸', fontsize=10)
                            plt.close(fig)  # í…ŒìŠ¤íŠ¸ í›„ ì¦‰ì‹œ ë‹«ê¸°
                            
                            logging.debug(f"í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ: {font_name}")
                            return True
                    except Exception as e:
                        logging.debug(f"í°íŠ¸ {font_name} ì„¤ì • ì‹¤íŒ¨: {e}")
                        continue
                        
            elif platform.system() == 'Darwin':  # macOS
                try:
                    plt.rcParams['font.family'] = 'AppleGothic'
                    plt.rcParams['axes.unicode_minus'] = False
                    logging.debug("í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ: AppleGothic")
                    return True
                except:
                    pass
                    
            elif platform.system() == 'Linux':
                try:
                    # Linuxì—ì„œ í•œê¸€ í°íŠ¸ ì‹œë„
                    linux_fonts = ['NanumGothic', 'NanumBarunGothic', 'DejaVu Sans']
                    for font_name in linux_fonts:
                        try:
                            plt.rcParams['font.family'] = font_name
                            plt.rcParams['axes.unicode_minus'] = False
                            logging.debug(f"í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ: {font_name}")
                            return True
                        except:
                            continue
                except:
                    pass
            
            # ê¸°ë³¸ ì„¤ì •ì´ ì‹¤íŒ¨í•œ ê²½ìš°
            logging.warning("í•œê¸€ í°íŠ¸ ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            plt.rcParams['axes.unicode_minus'] = False  # ìµœì†Œí•œ ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ëŠ” ë³´í˜¸
            return False
            
        except Exception as e:
            logging.error(f"í•œê¸€ í°íŠ¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
    
    # í•œê¸€ í°íŠ¸ ì„¤ì • ì‹¤í–‰
    setup_korean_font()
    
    # ğŸ†• í°íŠ¸ ìºì‹œ ìƒˆë¡œê³ ì¹¨ (í•„ìš”í•œ ê²½ìš°)
    try:
        # í°íŠ¸ ìºì‹œê°€ ì˜¤ë˜ëœ ê²½ìš° ìƒˆë¡œê³ ì¹¨
        fm._rebuild()
    except Exception as e:
        logging.debug(f"í°íŠ¸ ìºì‹œ ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨ (ë¬´ì‹œ ê°€ëŠ¥): {e}")
    
    # matplotlibì˜ ì›ë³¸ show í•¨ìˆ˜ ë°±ì—…
    original_show = plt.show
    original_clf = plt.clf
    original_cla = plt.cla
    original_close = plt.close
    
    def streamlit_show(*args, **kwargs):
        """
        plt.show()ë¥¼ Streamlit í™˜ê²½ì—ì„œ ìë™ìœ¼ë¡œ st.pyplot()ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
        """
        try:
            # í˜„ì¬ figureê°€ ìˆëŠ”ì§€ í™•ì¸
            fig = plt.gcf()
            if fig.get_axes():  # axesê°€ ìˆìœ¼ë©´ ì‹¤ì œ í”Œë¡¯ì´ ìˆë‹¤ëŠ” ì˜ë¯¸
                
                # ğŸ†• í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ë©”ì‹œì§€ì— ì‹œê°í™” ì¶”ê°€
                if "current_message_visualizations" not in st.session_state:
                    st.session_state.current_message_visualizations = []
                
                # figureë¥¼ base64 ì´ë¯¸ì§€ë¡œ ë³€í™˜
                import io
                import base64
                
                buf = io.BytesIO()
                fig.savefig(buf, format='png', bbox_inches='tight', dpi=100, facecolor='white')
                buf.seek(0)
                img_base64 = base64.b64encode(buf.getvalue()).decode()
                
                # HTML img íƒœê·¸ë¡œ ë³€í™˜ (Streamlit markdownì—ì„œ ë Œë”ë§ ê°€ëŠ¥)
                img_html = f'<img src="data:image/png;base64,{img_base64}" style="max-width:100%; height:auto; margin: 10px 0;">'
                
                # í˜„ì¬ ë©”ì‹œì§€ì˜ ì‹œê°í™” ëª©ë¡ì— ì¶”ê°€
                st.session_state.current_message_visualizations.append(img_html)
                
                # ğŸ†• ì‹œê°í™” ì „ìš© ì»¨í…Œì´ë„ˆì— í‘œì‹œ (ì‹¤ì‹œê°„)
                if hasattr(st, '_visualization_container') and st._visualization_container is not None:
                    with st._visualization_container:
                        st.pyplot(fig, clear_figure=False)
                else:
                    # ì¼ë°˜ì ì¸ ê²½ìš°
                    st.pyplot(fig, clear_figure=False)
                
                # ğŸ†• ìƒˆë¡œìš´ í”Œë¡¯ì„ ìœ„í•´ ìƒˆ figure ìƒì„±
                plt.figure()
                
            else:
                # ë¹ˆ figureì¸ ê²½ìš° ì›ë˜ show í•¨ìˆ˜ í˜¸ì¶œ
                original_show(*args, **kwargs)
        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë˜ show í•¨ìˆ˜ë¡œ fallback
            print(f"Streamlit show error: {e}")
            original_show(*args, **kwargs)
    
    def protected_clf(*args, **kwargs):
        """plt.clf()ë¥¼ ë³´í˜¸í•˜ì—¬ ì˜ë„ì¹˜ ì•Šì€ í´ë¦¬ì–´ ë°©ì§€"""
        # ìƒˆ figureë¥¼ ìƒì„±í•˜ë˜ ê¸°ì¡´ ê²ƒì€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ
        plt.figure()
    
    def protected_cla(*args, **kwargs):
        """plt.cla()ë¥¼ ë³´í˜¸í•˜ì—¬ ì˜ë„ì¹˜ ì•Šì€ í´ë¦¬ì–´ ë°©ì§€"""
        # í˜„ì¬ axesë§Œ í´ë¦¬ì–´í•˜ë˜ figureëŠ” ìœ ì§€
        if plt.gcf().get_axes():
            plt.gca().clear()
    
    def protected_close(*args, **kwargs):
        """plt.close()ë¥¼ ë³´í˜¸í•˜ì—¬ í‘œì‹œëœ figureëŠ” ìœ ì§€"""
        # ì¸ìê°€ ì—†ìœ¼ë©´ í˜„ì¬ figureë§Œ ë‹«ê¸°
        if not args and not kwargs:
            plt.figure()  # ìƒˆ figure ìƒì„±
        else:
            original_close(*args, **kwargs)
    
    # matplotlib show í•¨ìˆ˜ë¥¼ íŒ¨ì¹˜
    plt.show = streamlit_show
    
    # ğŸ†• matplotlib í´ë¦¬ì–´ í•¨ìˆ˜ë“¤ë„ íŒ¨ì¹˜í•˜ì—¬ ì˜ë„ì¹˜ ì•Šì€ figure ì‚­ì œ ë°©ì§€
    plt.clf = protected_clf
    plt.cla = protected_cla  
    plt.close = protected_close
    
    # ì¶”ê°€ ì‹œê°í™” í—¬í¼ í•¨ìˆ˜ë“¤
    def reset_show():
        """ì›ë³¸ matplotlib í•¨ìˆ˜ë“¤ë¡œ ë³µì›"""
        plt.show = original_show
        plt.clf = original_clf
        plt.cla = original_cla
        plt.close = original_close
    
    def force_show():
        """í˜„ì¬ figureë¥¼ ê°•ì œë¡œ Streamlitì— í‘œì‹œ"""
        fig = plt.gcf()
        if fig.get_axes():
            st.pyplot(fig, clear_figure=False)
            # ìƒˆë¡œìš´ figure ìƒì„± (ê¸°ì¡´ ê²ƒì€ ìœ ì§€)
            plt.figure()
    
    # ğŸ†• í•œê¸€ í°íŠ¸ ìƒíƒœ í™•ì¸ í•¨ìˆ˜
    def check_korean_font():
        """í˜„ì¬ ì„¤ì •ëœ í°íŠ¸ì™€ í•œê¸€ ì§€ì› ì—¬ë¶€ë¥¼ í™•ì¸"""
        current_font = plt.rcParams['font.family']
        unicode_minus = plt.rcParams['axes.unicode_minus']
        
        info = f"""
ğŸ“ **í°íŠ¸ ì„¤ì • ì •ë³´:**
- í˜„ì¬ í°íŠ¸: {current_font}
- ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ë³´í˜¸: {unicode_minus}
- í”Œë«í¼: {platform.system()}

ğŸ¨ **í•œê¸€ í…ŒìŠ¤íŠ¸**: ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ â† ì´ ê¸€ìë“¤ì´ ì •ìƒì ìœ¼ë¡œ ë³´ì´ë©´ ì„±ê³µ!
"""
        return info
    
    # ğŸ†• ë°ì´í„° ë¶„ì„ ì˜¤ë¥˜ ë³µêµ¬ìš© í—¬í¼ í•¨ìˆ˜ë“¤
    def safe_dataframe_check(obj):
        """DataFrameì„ ì•ˆì „í•˜ê²Œ ì²´í¬í•˜ëŠ” í•¨ìˆ˜"""
        if obj is None:
            return False
        if hasattr(obj, 'empty'):
            return not obj.empty
        return bool(obj)
    
    def diagnose_data(df=None):
        """ë°ì´í„° ì§„ë‹¨ ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
        if df is None and 'df' in locals():
            df = locals()['df']
        if df is None and 'data' in locals():
            df = locals()['data']
        if df is None:
            return "ì§„ë‹¨í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        try:
            info = f"""
ğŸ“Š ë°ì´í„° ì§„ë‹¨ ê²°ê³¼:
- í¬ê¸°: {df.shape[0]:,} í–‰ Ã— {df.shape[1]:,} ì—´  
- ì»¬ëŸ¼: {list(df.columns)}
- ë°ì´í„° íƒ€ì…: {dict(df.dtypes)}
- ê²°ì¸¡ê°’: {dict(df.isnull().sum())}
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum():,} bytes
"""
            return info
        except Exception as e:
            return f"ë°ì´í„° ì§„ë‹¨ ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    def safe_plot():
        """ì•ˆì „í•œ í”Œë¡¯ ìƒì„±ì„ ìœ„í•œ í•¨ìˆ˜"""
        try:
            fig = plt.gcf()
            if hasattr(fig, 'get_axes') and fig.get_axes():
                st.pyplot(fig, clear_figure=False)
                plt.figure()
                return "í”Œë¡¯ì´ ì„±ê³µì ìœ¼ë¡œ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤."
            else:
                return "í‘œì‹œí•  í”Œë¡¯ì´ ì—†ìŠµë‹ˆë‹¤."
        except Exception as e:
            return f"í”Œë¡¯ í‘œì‹œ ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    # ğŸ†• ì‹œê°í™” ê´€ë¦¬ìš© í—¬í¼ í•¨ìˆ˜ë“¤
    def get_current_visualizations():
        """í˜„ì¬ ë©”ì‹œì§€ì˜ ì‹œê°í™” ê°œìˆ˜ ë°˜í™˜"""
        if "current_message_visualizations" in st.session_state:
            return len(st.session_state.current_message_visualizations)
        return 0
    
    def clear_current_visualizations():
        """í˜„ì¬ ë©”ì‹œì§€ì˜ ì‹œê°í™” ë°ì´í„° ì œê±°"""
        if "current_message_visualizations" in st.session_state:
            count = len(st.session_state.current_message_visualizations)
            st.session_state.current_message_visualizations = []
            return f"{count}ê°œì˜ ì‹œê°í™” ë°ì´í„°ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤."
        return "ì œê±°í•  ì‹œê°í™” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    def preview_current_visualizations():
        """í˜„ì¬ ë©”ì‹œì§€ì˜ ì‹œê°í™”ë“¤ì„ ë¯¸ë¦¬ë³´ê¸°"""
        if ("current_message_visualizations" in st.session_state and 
            st.session_state.current_message_visualizations):
            st.write(f"**í˜„ì¬ ìƒì„±ëœ ì‹œê°í™” {len(st.session_state.current_message_visualizations)}ê°œ:**")
            for i, viz_html in enumerate(st.session_state.current_message_visualizations):
                st.markdown(f"ì‹œê°í™” {i+1}:", unsafe_allow_html=False)
                st.markdown(viz_html, unsafe_allow_html=True)
        else:
            st.write("í˜„ì¬ ìƒì„±ëœ ì‹œê°í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    analysis_env = {
        # ê¸°ë³¸ ë°ì´í„° ë¶„ì„ íŒ¨í‚¤ì§€ë“¤
        "pd": pd,
        "pandas": pd,
        "np": np,
        "numpy": np,
        "scipy": scipy,
        "sns": sns,
        "seaborn": sns,
        "plt": plt,
        "matplotlib": plt,
        
        # Streamlit 
        "st": st,
        
        # scikit-learn ê´€ë ¨
        "sklearn": sklearn,
        "datasets": datasets,
        "metrics": metrics,
        "model_selection": model_selection,
        "preprocessing": preprocessing,
        "linear_model": linear_model,
        "ensemble": ensemble,
        "cluster": cluster,
        
        # ê¸°íƒ€ ìœ ìš©í•œ íŒ¨í‚¤ì§€ë“¤
        "warnings": warnings,
        "os": os,
        "sys": sys,
        "json": json,
        "time": time,
        
        # ìì£¼ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ë“¤ì„ ì§ì ‘ ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ
        "train_test_split": model_selection.train_test_split,
        "StandardScaler": preprocessing.StandardScaler,
        "LinearRegression": linear_model.LinearRegression,
        "RandomForestClassifier": ensemble.RandomForestClassifier,
        "KMeans": cluster.KMeans,
        
        # ğŸ†• ì‹œê°í™” í—¬í¼ í•¨ìˆ˜ë“¤
        "reset_show": reset_show,
        "force_show": force_show,
        "original_show": original_show,
        "original_clf": original_clf,
        "original_cla": original_cla,
        "original_close": original_close,
        
        # ğŸ†• í°íŠ¸ ê´€ë ¨ í—¬í¼ í•¨ìˆ˜ë“¤
        "setup_korean_font": setup_korean_font,
        "check_korean_font": check_korean_font,
        
        # ğŸ†• ì˜¤ë¥˜ ë³µêµ¬ìš© í—¬í¼ í•¨ìˆ˜ë“¤
        "safe_dataframe_check": safe_dataframe_check,
        "diagnose_data": diagnose_data,
        "safe_plot": safe_plot,
        
        # ğŸ†• ì‹œê°í™” ê´€ë¦¬ìš© í—¬í¼ í•¨ìˆ˜ë“¤
        "get_current_visualizations": get_current_visualizations,
        "clear_current_visualizations": clear_current_visualizations,
        "preview_current_visualizations": preview_current_visualizations,
    }
    
    # DataFrameì´ ì œê³µëœ ê²½ìš° ì¶”ê°€
    if df is not None:
        analysis_env["df"] = df
        analysis_env["data"] = df  # ì¼ë°˜ì ì¸ ë³„ëª…ë„ ì¶”ê°€
    
    return analysis_env

# ğŸ†• ë°ì´í„° ë¶„ì„ìš© PythonAstREPLTool ìƒì„± í•¨ìˆ˜
def create_enhanced_python_tool(df=None):
    """
    ë°ì´í„° ë¶„ì„ íŒ¨í‚¤ì§€ë“¤ì´ ì‚¬ì „ ë¡œë“œëœ PythonAstREPLToolì„ ìƒì„±í•©ë‹ˆë‹¤.
    plt.show()ê°€ ìë™ìœ¼ë¡œ Streamlitì—ì„œ ë™ì‘í•˜ë„ë¡ íŒ¨ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    
    Args:
        df: ë¶„ì„í•  DataFrame (ì„ íƒì‚¬í•­)
    
    Returns:
        PythonAstREPLTool: í–¥ìƒëœ Python REPL ë„êµ¬
    """
    analysis_env = create_data_analysis_environment(df)
    
    # ì‚¬ìš©ì ì¹œí™”ì ì¸ ì„¤ëª…ê³¼ ì˜ˆì œ ì¶”ê°€
    description = """
    ğŸ¤– **ì§€ëŠ¥í˜• ë°ì´í„° ë¶„ì„ í™˜ê²½**ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!
    
    ğŸ“Š **ì‚¬ì „ ë¡œë“œëœ íŒ¨í‚¤ì§€ë“¤:**
    - ë°ì´í„° ì²˜ë¦¬: pandas (pd), numpy (np)
    - ì‹œê°í™”: matplotlib (plt), seaborn (sns), streamlit (st)  
    - ë¨¸ì‹ ëŸ¬ë‹: scikit-learn (sklearn)
    - ê³¼í•™ê³„ì‚°: scipy
    
    ğŸš€ **íŠ¹ë³„ ê¸°ëŠ¥ë“¤:**
    âœ… CSV ì—…ë¡œë“œ ì‹œ **ìˆ˜ë™ EDA(íƒìƒ‰ì  ë°ì´í„° ë¶„ì„)** ë²„íŠ¼ ì œê³µ
    âœ… **ë°ì´í„° íŠ¹ì„± ìë™ ë¶„ì„** ë° **ë¶„ì„ ë°©í–¥ ì¶”ì²œ**  
    âœ… plt.show() ìë™ Streamlit ë³€í™˜ (ì‹œê°í™” ì˜êµ¬ ë³´ì¡´)
    âœ… **ë§ì¶¤í˜• í›„ì† ì§ˆë¬¸** ìë™ ìƒì„±
    âœ… plt.clf(), plt.cla(), plt.close() ë“± í´ë¦¬ì–´ í•¨ìˆ˜ë“¤ë¡œë¶€í„° ë³´í˜¸
    âœ… ë„êµ¬ í˜¸ì¶œ ì •ë³´ê°€ ì ‘í˜€ë„ ì‹œê°í™”ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€!
    
    ğŸ“ˆ **ìë™ ë¶„ì„ í•­ëª©:**
    - ë°ì´í„° í¬ê¸°, íƒ€ì…, ê²°ì¸¡ê°’ í˜„í™©
    - ìˆ˜ì¹˜í˜•/ë²”ì£¼í˜•/ë‚ ì§œí˜• ì»¬ëŸ¼ ë¶„ë¥˜  
    - ê¸°ë³¸ í†µê³„ ìš”ì•½ ë° ë¶„í¬ ì‹œê°í™”
    - ë¶„ì„ ìš°ì„ ìˆœìœ„ ë° ë°©í–¥ ì œì•ˆ
    
    ğŸ¯ **ì‹œì‘ ë°©ë²•:**
    1. CSV íŒŒì¼ ì—…ë¡œë“œ â†’ Agent ìƒì„±
    2. ì‚¬ì´ë“œë°”ì˜ 'ğŸš€ ìë™ ë°ì´í„° ë¶„ì„ ì‹œì‘' ë²„íŠ¼ í´ë¦­
    3. ì œì•ˆëœ ë¶„ì„ ì¤‘ ì›í•˜ëŠ” ê²ƒ ì„ íƒ
    4. ëŒ€í™”í˜•ìœ¼ë¡œ ì‹¬í™” ë¶„ì„ ì§„í–‰
    
    ğŸ’¬ **ì‚¬ìš© ì˜ˆì‹œ:**
    - "ê²°ì¸¡ê°’ì„ ì²˜ë¦¬í•´ì¤˜"
    - "ìƒê´€ê´€ê³„ë¥¼ ì‹œê°í™”í•´ì¤˜" 
    - "ì´ìƒì¹˜ë¥¼ ì°¾ì•„ì¤˜"
    - "í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ì„ í•´ì¤˜"
    
    DataFrameì€ 'df' ë˜ëŠ” 'data' ë³€ìˆ˜ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ë¬´ì—‡ì„ ë¶„ì„í•˜ê³  ì‹¶ìœ¼ì‹ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”! ğŸ¤–âœ¨
    """
    
    return PythonAstREPLTool(
        locals=analysis_env,
        description=description,
        name="enhanced_python_repl",
        handle_tool_error=True
    )

# ğŸ†• ìë™ ë°ì´í„° ë¶„ì„ ë° ì¸ì‚¬ë§ ìƒì„± í•¨ìˆ˜
def auto_analyze_and_greet(df):
    """
    ë°ì´í„° ë¡œë“œ ì‹œ ìë™ìœ¼ë¡œ ê¸°ë³¸ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ì¸ì‚¬ë§ê³¼ ê°€ì´ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        # ë°ì´í„° ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        shape = df.shape
        columns = df.columns.tolist()
        dtypes = df.dtypes.value_counts().to_dict()
        missing_values = df.isnull().sum().sum()
        missing_cols = df.isnull().sum()[df.isnull().sum() > 0].to_dict()
        
        # ìˆ˜ì¹˜í˜•/ë²”ì£¼í˜• ì»¬ëŸ¼ ë¶„ë¥˜
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        datetime_cols = df.select_dtypes(include=['datetime']).columns.tolist()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory_usage = df.memory_usage(deep=True).sum() / 1024 / 1024  # MB
        
        # ì¸ì‚¬ë§ ë° ë¶„ì„ ê²°ê³¼ ìƒì„±
        greeting_content = f"""ğŸ‰ **ë°ì´í„° ë¶„ì„ í™˜ê²½ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!**

ğŸ“Š **ë¡œë“œëœ ë°ì´í„° ê°œìš”:**
- **ë°ì´í„° í¬ê¸°**: {shape[0]:,} í–‰ Ã— {shape[1]:,} ì—´
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {memory_usage:.2f} MB
- **ê²°ì¸¡ê°’**: {missing_values:,} ê°œ ({missing_values/df.size*100:.1f}%)

ğŸ“‹ **ì»¬ëŸ¼ êµ¬ì„±:**
- **ìˆ˜ì¹˜í˜• ì»¬ëŸ¼** ({len(numeric_cols)}ê°œ): {', '.join(numeric_cols[:5])}{'...' if len(numeric_cols) > 5 else ''}
- **ë²”ì£¼í˜• ì»¬ëŸ¼** ({len(categorical_cols)}ê°œ): {', '.join(categorical_cols[:5])}{'...' if len(categorical_cols) > 5 else ''}
{'- **ë‚ ì§œí˜• ì»¬ëŸ¼** (' + str(len(datetime_cols)) + 'ê°œ): ' + ', '.join(datetime_cols[:3]) + ('...' if len(datetime_cols) > 3 else '') if datetime_cols else ''}

ğŸ” **ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:**"""

        # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬í•¨
        preview_text = df.head(3).to_string()
        greeting_content += f"\n```\n{preview_text}\n```\n"
        
        # ë¶„ì„ ì œì•ˆ ìƒì„±
        suggestions = []
        
        if missing_values > 0:
            suggestions.append(f"ğŸ“ **ê²°ì¸¡ê°’ ì²˜ë¦¬**: {len(missing_cols)}ê°œ ì»¬ëŸ¼ì— ê²°ì¸¡ê°’ì´ ìˆìŠµë‹ˆë‹¤")
            
        if len(numeric_cols) >= 2:
            suggestions.append("ğŸ“ˆ **ìƒê´€ê´€ê³„ ë¶„ì„**: ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”")
            
        if len(categorical_cols) > 0:
            suggestions.append("ğŸ“Š **ë²”ì£¼í˜• ë°ì´í„° ë¶„í¬**: ì¹´í…Œê³ ë¦¬ë³„ ë¹ˆë„ì™€ ë¶„í¬ë¥¼ ì‚´í´ë³´ì„¸ìš”")
            
        if len(numeric_cols) > 0:
            suggestions.append("ğŸ“‰ **ê¸°ì´ˆ í†µê³„**: ìˆ˜ì¹˜í˜• ë°ì´í„°ì˜ ë¶„í¬ì™€ ì´ìƒì¹˜ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”")
            
        if shape[0] > 1000:
            suggestions.append("ğŸ¯ **ìƒ˜í”Œë§**: í° ë°ì´í„°ì…‹ì´ë¯€ë¡œ ìƒ˜í”Œë§ì„ ê³ ë ¤í•´ë³´ì„¸ìš”")
        
        # followupsë¥¼ suggestionsì—ì„œ ë™ì ìœ¼ë¡œ ìƒì„±
        followups = [
            s.split(":")[0]
            .replace("ğŸ“", "")
            .replace("ğŸ“ˆ", "")
            .replace("ğŸ“Š", "")
            .replace("ğŸ“‰", "")
            .replace("ğŸ¯", "")
            .replace("**", "")
            .strip()
            for s in suggestions
        ]
        if not followups:
            followups = ["ë°ì´í„°ì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ë³´ì—¬ì¤˜", "ì²« 5í–‰ì„ ë³´ì—¬ì¤˜", "ë°ì´í„° ìš”ì•½ í†µê³„ë¥¼ ë³´ì—¬ì¤˜"]

        # êµ¬ì²´ì ì¸ ë¶„ì„ ëª…ë ¹ì–´ ì œì•ˆ
        greeting_content += """
ğŸš€ **ë¹ ë¥¸ ì‹œì‘ ëª…ë ¹ì–´:**
- `df.describe()` - ê¸°ì´ˆ í†µê³„ ìš”ì•½
- `df.info()` - ë°ì´í„° íƒ€ì… ë° ê²°ì¸¡ê°’ ì •ë³´  
- `df.hist(figsize=(12, 8)); plt.show()` - ì „ì²´ ë³€ìˆ˜ íˆìŠ¤í† ê·¸ë¨
- `sns.heatmap(df.corr(), annot=True); plt.show()` - ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
- `df.isnull().sum()` - ê²°ì¸¡ê°’ í™•ì¸

ë¬´ì—‡ì„ ë¶„ì„í•˜ê³  ì‹¶ìœ¼ì‹ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”! ğŸ¤–âœ¨"""

        # ì¶”ì²œ ë¶„ì„ ë‹¨ê³„ ë¸”ë¡ì„ ë§¨ ë§ˆì§€ë§‰ì— ì¶”ê°€
        greeting_content += "\n\nğŸ’¡ **ì¶”ì²œ ë¶„ì„ ë‹¨ê³„:**\n"
        for i, suggestion in enumerate(suggestions[:4], 1):
            greeting_content += f"{i}. {suggestion}\n"
        
        # ì‹œê°í™” ìƒì„±ì„ ìœ„í•œ ê¸°ë³¸ í”Œë¡¯
        visualizations = []
        try:
            # ê°„ë‹¨í•œ ë°ì´í„° ê°œìš” ì‹œê°í™” ìƒì„±
            import matplotlib.pyplot as plt
            import seaborn as sns
            
            # 1. ë°ì´í„° íƒ€ì… ë¶„í¬ íŒŒì´ì°¨íŠ¸
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
            
            # ì»¬ëŸ¼ íƒ€ì… ë¶„í¬
            type_counts = {'ìˆ˜ì¹˜í˜•': len(numeric_cols), 'ë²”ì£¼í˜•': len(categorical_cols), 'ë‚ ì§œí˜•': len(datetime_cols)}
            type_counts = {k: v for k, v in type_counts.items() if v > 0}
            
            ax1.pie(type_counts.values(), labels=type_counts.keys(), autopct='%1.1f%%', startangle=90)
            ax1.set_title('ì»¬ëŸ¼ íƒ€ì… ë¶„í¬')
            
            # ê²°ì¸¡ê°’ í˜„í™©
            if missing_values > 0 and len(missing_cols) <= 10:
                missing_data = pd.Series(missing_cols)
                missing_data.plot(kind='bar', ax=ax2, color='coral')
                ax2.set_title('ì»¬ëŸ¼ë³„ ê²°ì¸¡ê°’ ê°œìˆ˜')
                ax2.set_ylabel('ê²°ì¸¡ê°’ ê°œìˆ˜')
                plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')
            else:
                ax2.text(0.5, 0.5, f'ì „ì²´ ê²°ì¸¡ê°’: {missing_values:,}ê°œ\n({missing_values/df.size*100:.1f}%)', 
                        ha='center', va='center', transform=ax2.transAxes, fontsize=12)
                ax2.set_title('ê²°ì¸¡ê°’ í˜„í™©')
                ax2.axis('off')
            
            plt.tight_layout()
            
            # figureë¥¼ base64ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
            import io
            import base64
            
            buf = io.BytesIO()
            fig.savefig(buf, format='png', bbox_inches='tight', dpi=100, facecolor='white')
            buf.seek(0)
            img_base64 = base64.b64encode(buf.getvalue()).decode()
            img_html = f'<img src="data:image/png;base64,{img_base64}" style="max-width:100%; height:auto; margin: 10px 0;">'
            
            visualizations.append(img_html)
            plt.close(fig)  # ë©”ëª¨ë¦¬ ì •ë¦¬
            
        except Exception as viz_error:
            logging.warning(f"ì´ˆê¸° ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {viz_error}")
        
        # ë¶„ì„ ê²°ê³¼ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥ (historyì—ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ)
        analysis_result = {
            "content": greeting_content,
            "visualizations": visualizations,
            "followups": followups
        }
        
        # ğŸ†• ë¶„ì„ ê²°ê³¼ë¥¼ ë³„ë„ ì €ì¥ (ì´ˆê¸°í™” ì™„ë£Œ í›„ ì‚¬ìš©)
        st.session_state.auto_analysis_result = analysis_result
        
        return True
        
    except Exception as e:
        logging.error(f"ìë™ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        # ê°„ë‹¨í•œ ì¸ì‚¬ë§ë§Œ ì €ì¥
        simple_greeting = f"""ğŸ‰ **ë°ì´í„° ë¶„ì„ í™˜ê²½ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!**

ğŸ“Š **ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤**: {df.shape[0]:,} í–‰ Ã— {df.shape[1]:,} ì—´

ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì‹¶ì€ ë‚´ìš©ì„ ë§ì”€í•´ ì£¼ì„¸ìš”! ğŸ¤–âœ¨"""
        
        st.session_state.auto_analysis_result = {
            "content": simple_greeting,
            "visualizations": [],
            "followups": ["ë°ì´í„°ì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ë³´ì—¬ì¤˜", "ì²« 5í–‰ì„ ë³´ì—¬ì¤˜", "ë°ì´í„° ìš”ì•½ í†µê³„ë¥¼ ë³´ì—¬ì¤˜"]
        }
        
        return False

# Log function entry and exit
logging.debug('Entering function: initialize_session')
async def initialize_session(mcp_config=None):
    logging.debug('Initializing MCP session')
    with st.spinner("ğŸ”„ MCP ì„œë²„ì— ì—°ê²° ì¤‘..."):
        await cleanup_mcp_client()
        logging.debug('MCP client cleaned up')

        # mcp_configì´ Noneì´ê±°ë‚˜ tool_configê°€ ì—†ëŠ” ê²½ìš° MCP ì—°ê²°ì„ ê±´ë„ˆëœë‹ˆë‹¤.
        if mcp_config is None and (
            "tool_config" not in st.session_state or st.session_state.tool_config is None
        ):
            st.warning("âš ï¸ MCP ì„œë²„ ì—°ê²°ì„ ê±´ë„ˆëœë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ MCP Toolì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            st.session_state.tool_count = 0
            st.session_state.mcp_client = None
            st.session_state.session_initialized = True
            logging.debug('No tool configuration found, skipping MCP connection.')
            return True

        # mcp_configì´ Noneì´ë©´ ì‚¬ì´ë“œë°”ì—ì„œ ë¡œë“œëœ tool_config ì‚¬ìš©
        if mcp_config is None:
            mcp_config = st.session_state.tool_config

        # mcpServers í‚¤ê°€ ìˆìœ¼ë©´ í•´ì œ
        connections = mcp_config.get("mcpServers", mcp_config)
        
        # Store connections for debugging
        st.session_state.last_mcp_connections = connections
        logging.debug(f"MCP connections configuration: {json.dumps(connections, indent=2)}")
        
        # MCP ì„œë²„ ì„¤ì •ì´ ë¹„ì–´ ìˆìœ¼ë©´ ê±´ë„ˆëœë‹ˆë‹¤.
        if not connections:
            st.warning("âš ï¸ MCP ì„œë²„ ì„¤ì •ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. MCP ì—°ê²°ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            st.session_state.tool_count = 0
            st.session_state.mcp_client = None
            st.session_state.session_initialized = True
            logging.debug('MCP server configuration is empty, skipping connection.')
            return True

        # Initialize MCP client and connect to servers
        try:
            logging.debug("Creating MultiServerMCPClient with connections")
            client = MultiServerMCPClient(connections)
            
            logging.debug("Entering MCP client context")
            await client.__aenter__()
            logging.debug('MCP servers connected via context manager.')
            
            try:
                # Get and log available tools
                logging.debug("Retrieving tools from MCP client")
                tools = client.get_tools()
                tool_count = len(tools)
                st.session_state.tool_count = tool_count
                
                # Log individual tool details
                logging.debug(f"Retrieved {tool_count} tools from MCP client")
                for i, tool in enumerate(tools):
                    tool_name = getattr(tool, 'name', f"Tool_{i}")
                    logging.debug(f"Tool {i}: {tool_name}")
                    if hasattr(tool, 'args'):
                        logging.debug(f"Tool {i} args: {tool.args}")
                    if hasattr(tool, 'description'):
                        logging.debug(f"Tool {i} description: {tool.description}")
                
                st.session_state.mcp_client = client
                
            except Exception as e:
                logging.error(f"Error retrieving tools: {str(e)}")
                import traceback
                logging.error(f"Tool retrieval error details:\n{traceback.format_exc()}")
                st.error(f"MCP ë„êµ¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                # Continue with empty tools list
                tools = []
                tool_count = 0
                st.session_state.tool_count = 0
            
            # Create agent based on whether DataFrame is available
            if tool_count > 0 or st.session_state.dataframe is not None:
                # Replace HTTPChatModel usage with ChatOpenAI
                load_dotenv()
                # Construct OpenAI API base URL (always host + /v1)
                raw_base = os.getenv("OPENAI_API_BASE", "https://api.openai.com")
                # Remove any path, keep only scheme and netloc
                parsed = urlsplit(raw_base)
                openai_api_base = f"{parsed.scheme}://{parsed.netloc}/v1"
                openai_api_key = os.getenv("OPENAI_API_KEY", "")
                logging.debug(f"Creating ChatOpenAI with base_url: {openai_api_base}")
                model = ChatOpenAI(
                    model=st.session_state.selected_model,
                    temperature=st.session_state.temperature,
                    max_tokens=OUTPUT_TOKEN_INFO[st.session_state.selected_model]["max_tokens"],
                    api_key=openai_api_key,
                    base_url=openai_api_base,
                    streaming=True,  # Enable streaming
                )
                try:
                    # --- ì—ì´ì „íŠ¸ ìƒì„± ---
                    if st.session_state.dataframe is not None:          # DataFrameì´ ìˆìœ¼ë©´
                        df = st.session_state.dataframe
                        
                        # ğŸ†• ë°ì´í„° ë¶„ì„ìš© ë„êµ¬ ìƒì„±
                        enhanced_python_tool = create_enhanced_python_tool(df)
                        
                        # ğŸ”§ ìˆ˜ì •: MCP ë„êµ¬ë“¤ì„ pandas agentìš© ë™ê¸° í˜¸í™˜ìœ¼ë¡œ ë³€í™˜
                        extra_tools = [enhanced_python_tool]
                        if tools:
                            # MCP ë„êµ¬ë“¤ì„ ë™ê¸° í˜¸í™˜ìœ¼ë¡œ ë³€í™˜
                            sync_compatible_tools = []
                            for tool in tools:
                                try:
                                    sync_tool = make_mcp_tool_sync_compatible(tool)
                                    sync_compatible_tools.append(sync_tool)
                                    logging.debug(f"Successfully made {tool.name} sync compatible")
                                except Exception as e:
                                    logging.error(f"Failed to make {tool.name} sync compatible: {e}")
                                    # ì‹¤íŒ¨í•œ ë„êµ¬ëŠ” ê±´ë„ˆë›°ê¸°
                                    continue
                            
                            extra_tools.extend(sync_compatible_tools)
                            logging.debug(f"Adding {len(sync_compatible_tools)} sync-compatible MCP tools to pandas agent")
                        
                        # Create pandas agent with sync-compatible MCP tools
                        pandas_agent = create_pandas_dataframe_agent(
                            model,
                            df,
                            verbose=True,
                            agent_type=AgentType.OPENAI_FUNCTIONS,
                            allow_dangerous_code=True,
                            prefix=st.session_state.selected_prompt_text,
                            handle_parsing_errors=True,
                            max_iterations=10,
                            early_stopping_method="generate",
                            extra_tools=extra_tools  # ë™ê¸° í˜¸í™˜ MCP ë„êµ¬ë“¤ ì¶”ê°€
                        )
                        
                        # ğŸ”§ ë¹„ë™ê¸° ë˜í¼ë¡œ ê°ì‹¸ê¸°
                        async_pandas_wrapper = AsyncPandasAgentWrapper(pandas_agent)
                        
                        st.session_state.agent = async_pandas_wrapper
                        st.session_state.agent_type = "pandas"
                        logging.debug(f'Enhanced async pandas agent with {len(extra_tools)} total tools created successfully')
                        
                        # ğŸ†• ì‚¬ìš© ê°€ëŠ¥í•œ íŒ¨í‚¤ì§€ ì •ë³´ë¥¼ ì‚¬ìš©ìì—ê²Œ í‘œì‹œ
                        st.sidebar.success("âœ… ì§€ëŠ¥í˜• ë°ì´í„° ë¶„ì„ í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ!")
                        
                        with st.sidebar.expander("ğŸ“¦ ì‚¬ì „ ë¡œë“œëœ íŒ¨í‚¤ì§€", expanded=False):
                            st.write(f"""
                            **ë°ì´í„° ì²˜ë¦¬:**
                            - pandas (pd), numpy (np)
                            
                            **ì‹œê°í™”:**
                            - matplotlib (plt), seaborn (sns)
                            - âœ¨ plt.show() ìë™ Streamlit ë³€í™˜ (ì˜êµ¬ ë³´ì¡´)
                            
                            **ë¨¸ì‹ ëŸ¬ë‹:**
                            - scikit-learn (sklearn)
                            - datasets, metrics, model_selection
                            - preprocessing, linear_model, ensemble, cluster
                            
                            **ê³¼í•™ê³„ì‚°:**
                            - scipy
                            
                            **MCP ë„êµ¬ (ë™ê¸° í˜¸í™˜):**
                            - {len(tools)}ê°œì˜ ì™¸ë¶€ ë„êµ¬ í†µí•© (pandas agent í˜¸í™˜)
                            
                            **ì¶”ì²œ ì‹œì‘ ëª…ë ¹ì–´:**
                            - `df.describe()` - ê¸°ì´ˆ í†µê³„ ìš”ì•½
                            - `df.hist(); plt.show()` - íˆìŠ¤í† ê·¸ë¨
                            - `sns.heatmap(df.corr()); plt.show()` - ìƒê´€ê´€ê³„
                            """)
                        
                    else:                                               # DataFrameì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ReAct ìœ ì§€
                        # ğŸ†• ì¼ë°˜ ì—ì´ì „íŠ¸ì—ë„ í–¥ìƒëœ Python ë„êµ¬ ì¶”ê°€
                        enhanced_tools = tools.copy() if tools else []
                        enhanced_python_tool = create_enhanced_python_tool()
                        enhanced_tools.append(enhanced_python_tool)
                        if tools:
                            sync_compatible_tools = []
                            for tool in tools:
                                try:
                                    sync_tool = make_mcp_tool_sync_compatible(tool)
                                    sync_compatible_tools.append(sync_tool)
                                    logging.debug(f"Successfully made {tool.name} sync compatible")
                                except Exception as e:
                                    logging.error(f"Failed to make {tool.name} sync compatible: {e}")
                                    continue
                            enhanced_tools.extend(sync_compatible_tools)
                        
                        agent = create_react_agent(
                            model,
                            enhanced_tools,
                            prompt=st.session_state.selected_prompt_text,
                            checkpointer=MemorySaver(),
                        )
                        st.session_state.agent = agent
                        st.session_state.agent_type = "langgraph"
                        logging.debug('Enhanced LangGraph ReAct agent created successfully')
                        
                except Exception as e:
                    logging.error(f"Error creating agent: {str(e)}")
                    import traceback
                    logging.error(f"Agent creation error details:\n{traceback.format_exc()}")
                    st.error(f"ì—ì´ì „íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    st.session_state.agent = None
                    st.session_state.agent_type = None
            else:
                st.session_state.agent = None
                st.session_state.agent_type = None
                logging.warning('No tools available and no DataFrame loaded, agent not created.')
            
            st.session_state.session_initialized = True
            return True
            
        except Exception as e:
            import traceback
            error_msg = f"Error initializing MCP client: {str(e)}"
            logging.error(f"{error_msg}\n{traceback.format_exc()}")
            st.error(error_msg)
            st.session_state.session_initialized = False
            return False

# Log function entry and exit
logging.debug('Entering function: cleanup_mcp_client')
async def cleanup_mcp_client():
    """
    ê¸°ì¡´ MCP í´ë¼ì´ì–¸íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œí•©ë‹ˆë‹¤.

    ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ê°€ ìˆëŠ” ê²½ìš° ì •ìƒì ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ë¥¼ í•´ì œí•©ë‹ˆë‹¤.
    """
    if "mcp_client" in st.session_state and st.session_state.mcp_client is not None:
        try:
            await st.session_state.mcp_client.__aexit__(None, None, None)
            st.session_state.mcp_client = None
        except Exception as e:
            import traceback
            logging.error(f"MCP client cleanup error: {str(e)}\n{traceback.format_exc()}")
    logging.debug('Exiting function: cleanup_mcp_client')


def print_message():
    """
    ì±„íŒ… ê¸°ë¡ì„ í™”ë©´ì— ì¶œë ¥í•©ë‹ˆë‹¤.

    ì‚¬ìš©ìì™€ ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ë©”ì‹œì§€ë¥¼ êµ¬ë¶„í•˜ì—¬ í™”ë©´ì— í‘œì‹œí•˜ê³ ,
    ë„êµ¬ í˜¸ì¶œ ì •ë³´ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì»¨í…Œì´ë„ˆ ë‚´ì— í‘œì‹œí•©ë‹ˆë‹¤.
    """
    i = 0
    while i < len(st.session_state.history):
        message = st.session_state.history[i]

        if message["role"] == "user":
            st.chat_message("user", avatar="ğŸ§‘ğŸ»").write(message["content"])
            i += 1
        elif message["role"] == "assistant":
            # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì»¨í…Œì´ë„ˆ ìƒì„±
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                # ğŸ†• ë©”ì‹œì§€ ë‚´ìš©ì„ HTMLë¡œ ë Œë”ë§ (ì‹œê°í™” í¬í•¨)
                content = message["content"]
                
                # ì‹œê°í™”ê°€ í¬í•¨ëœ ê²½ìš° HTMLë¡œ ë Œë”ë§
                if "visualizations" in message and message["visualizations"]:
                    # í…ìŠ¤íŠ¸ ë‚´ìš© ë¨¼ì € í‘œì‹œ
                    if content and content.strip():
                        st.write(content)
                    
                    # ì‹œê°í™”ë“¤ì„ HTMLë¡œ í‘œì‹œ
                    for viz_html in message["visualizations"]:
                        st.markdown(viz_html, unsafe_allow_html=True)
                else:
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš°
                    st.write(content)

                # --- Followup ë²„íŠ¼ ë Œë”ë§ ---
                followups = message.get("followups")
                if followups:
                    st.markdown("<div style='margin-top: 0.5em; margin-bottom: 0.5em; color: #888;'>í›„ì† ì§ˆë¬¸ ì œì•ˆ:</div>", unsafe_allow_html=True)
                    btn_cols = st.columns(len(followups))
                    for idx, followup in enumerate(followups):
                        if btn_cols[idx].button(followup, key=f"followup_{i}_{idx}"):
                            st.session_state["user_query"] = followup
                            st.rerun()

                # ë‹¤ìŒ ë©”ì‹œì§€ê°€ ë„êµ¬ í˜¸ì¶œ ì •ë³´ì¸ì§€ í™•ì¸
                if (
                    i + 1 < len(st.session_state.history)
                    and st.session_state.history[i + 1]["role"] == "assistant_tool"
                ):
                    # ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ ë™ì¼í•œ ì»¨í…Œì´ë„ˆ ë‚´ì— expanderë¡œ í‘œì‹œ
                    with st.expander("ğŸ”§ ë„êµ¬ í˜¸ì¶œ ì •ë³´", expanded=False):
                        st.write(st.session_state.history[i + 1]["content"])
                    i += 2  # ë‘ ë©”ì‹œì§€ë¥¼ í•¨ê»˜ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ 2 ì¦ê°€
                else:
                    i += 1  # ì¼ë°˜ ë©”ì‹œì§€ë§Œ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ 1 ì¦ê°€
        else:
            # assistant_tool ë©”ì‹œì§€ëŠ” ìœ„ì—ì„œ ì²˜ë¦¬ë˜ë¯€ë¡œ ê±´ë„ˆëœ€
            i += 1


def get_streaming_callback(text_placeholder, tool_placeholder):
    """
    ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¨ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (ê°œì„ ëœ ë²„ì „)

    ì´ í•¨ìˆ˜ëŠ” LLMì—ì„œ ìƒì„±ë˜ëŠ” ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™”ë©´ì— í‘œì‹œí•˜ê¸° ìœ„í•œ ì½œë°± í•¨ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    í…ìŠ¤íŠ¸ ì‘ë‹µê³¼ ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ ê°ê° ë‹¤ë¥¸ ì˜ì—­ì— í‘œì‹œí•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        text_placeholder: í…ìŠ¤íŠ¸ ì‘ë‹µì„ í‘œì‹œí•  Streamlit ì»´í¬ë„ŒíŠ¸
        tool_placeholder: ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ í‘œì‹œí•  Streamlit ì»´í¬ë„ŒíŠ¸

    ë°˜í™˜ê°’:
        callback_func: ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¨ìˆ˜
        accumulated_text: ëˆ„ì ëœ í…ìŠ¤íŠ¸ ì‘ë‹µì„ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸
        accumulated_tool: ëˆ„ì ëœ ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸
    """
    accumulated_text = []
    accumulated_tool = []
    # Track tool call IDs to prevent duplicate pending calls
    seen_tool_call_ids = set()
    
    # ğŸ†• pandas agentìš© ìŠ¤íŠ¸ë¦¼ íŒŒì„œ ì´ˆê¸°í™” (utils.pyì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜ë“¤ ì‚¬ìš©)
    pandas_callbacks = AgentCallbacks(
        tool_callback=lambda tool: _handle_pandas_tool_display(tool, accumulated_tool, tool_placeholder),
        observation_callback=lambda obs: _handle_pandas_observation_display(obs, accumulated_tool, tool_placeholder),
        result_callback=lambda result: _handle_pandas_result_display(result, accumulated_text, text_placeholder)
    )
    pandas_parser = PandasAgentStreamParser(pandas_callbacks)

    def callback_func(message: dict):
        nonlocal accumulated_text, accumulated_tool
        logging.debug(f"Streaming callback received message: {type(message)}")
        message_content = message.get("content", None)
        
        # Log message content for debugging
        if message_content:
            logging.debug(f"Message content type: {type(message_content)}")
            if hasattr(message_content, "content"):
                content_sample = str(message_content.content)[:100] + "..." if len(str(message_content.content)) > 100 else str(message_content.content)
                logging.debug(f"Content: {content_sample}")
        else:
            logging.debug("No message content found")
        
        # Extract node and content from the message
        node = message.get("node", "")
        
        # ğŸ†• pandas agent streaming ì²˜ë¦¬ - utils.pyì˜ PandasAgentStreamParser ì‚¬ìš©
        if isinstance(message_content, dict) and {"actions", "steps"} & message_content.keys():
            try:
                pandas_parser.process_agent_steps(message_content)
                return None
            except Exception as e:
                logging.warning(f"Error in pandas agent stream parser: {e}")
                # Fallback to original processing
        
        # Handle pandas agent streaming - process step-by-step execution (fallback)
        if isinstance(message_content, dict):
            # Handle pandas agent steps
            if "actions" in message_content:
                actions = message_content["actions"]
                for action in actions:
                    tool_name = getattr(action, "tool", "Unknown Tool")
                    tool_input = getattr(action, "tool_input", {})
                    
                    # Display tool execution
                    if tool_name == "python_repl_ast" or tool_name == "enhanced_python_repl":  # ğŸ†• í–¥ìƒëœ ë„êµ¬ ì´ë¦„ë„ ì²´í¬
                        query = tool_input.get("query", "")
                        entry = f"\n**ğŸ Python ì½”ë“œ ì‹¤í–‰ (Enhanced):**\n```python\n{query}\n```\n"
                        accumulated_tool.append(entry)
                        tool_placeholder.markdown("".join(accumulated_tool))
                        time.sleep(0.01)
                        logging.debug(f"Added Enhanced Python code execution: {query[:100]}...")
                    else:
                        entry = f"\n**ë„êµ¬ í˜¸ì¶œ: {tool_name}**\nì…ë ¥: {json.dumps(tool_input, indent=2, ensure_ascii=False)}\n"
                        accumulated_tool.append(entry)
                        tool_placeholder.markdown("".join(accumulated_tool))
                        time.sleep(0.01)
                        logging.debug(f"Added tool call: {tool_name}")
                return None
                
            # Handle pandas agent observations
            if "steps" in message_content:
                steps = message_content["steps"]
                for step in steps:
                    obs = getattr(step, "observation", None)
                    # ğŸ†• DataFrame í˜¸í™˜ì„±ì„ ìœ„í•œ ì•ˆì „í•œ None ì²´í¬
                    if obs is not None and not (hasattr(obs, 'empty') and obs.empty):
                        # Format observation output
                        obs_str = str(obs)
                        if len(obs_str) > 1000:
                            obs_str = obs_str[:1000] + "..."
                        entry = f"\n**ğŸ“Š ì‹¤í–‰ ê²°ê³¼:**\n```\n{obs_str}\n```\n"
                        accumulated_tool.append(entry)
                        tool_placeholder.markdown("".join(accumulated_tool))
                        time.sleep(0.01)
                        logging.debug(f"Added observation result: {obs_str[:100]}...")
                return None
                
            # Handle final output from pandas agent
            if "output" in message_content:
                output = message_content["output"]
                if isinstance(output, str) and output.strip():
                    accumulated_text.append(output)
                    text_placeholder.markdown("".join(accumulated_text))
                    time.sleep(0.01)
                    logging.debug(f"Added final output: {output[:100]}...")
                return None
                
            # Handle intermediate_steps for pandas agent
            if "intermediate_steps" in message_content:
                steps = message_content["intermediate_steps"]
                for step in steps:
                    if hasattr(step, "action") and hasattr(step, "observation"):
                        # AgentStep í˜•íƒœ
                        action = step.action
                        observation = step.observation
                        
                        # ì•¡ì…˜ ì²˜ë¦¬
                        if hasattr(action, "tool"):
                            tool_name = getattr(action, "tool", "Unknown Tool")
                            tool_input = getattr(action, "tool_input", {})
                            
                            if tool_name == "python_repl_ast" or tool_name == "enhanced_python_repl":  # ğŸ†•
                                query = tool_input.get("query", "")
                                entry = f"\n**ğŸ Python ì½”ë“œ ì‹¤í–‰ (Enhanced):**\n```python\n{query}\n```\n"
                            else:
                                entry = f"\n**ë„êµ¬ í˜¸ì¶œ: {tool_name}**\nì…ë ¥: {json.dumps(tool_input, indent=2, ensure_ascii=False)}\n"
                            accumulated_tool.append(entry)
                        
                        # ê´€ì°° ì²˜ë¦¬ - ğŸ†• DataFrame í˜¸í™˜ì„±ì„ ìœ„í•œ ì•ˆì „í•œ ì²´í¬
                        if observation is not None and not (hasattr(observation, 'empty') and observation.empty):
                            obs_str = str(observation)
                            if len(obs_str) > 1000:
                                obs_str = obs_str[:1000] + "..."
                            entry = f"\n**ğŸ“Š ì‹¤í–‰ ê²°ê³¼:**\n```\n{obs_str}\n```\n"
                            accumulated_tool.append(entry)
                    elif isinstance(step, tuple) and len(step) == 2:
                        # (action, observation) íŠœí”Œ í˜•íƒœ
                        action, observation = step
                        
                        if hasattr(action, "tool"):
                            tool_name = getattr(action, "tool", "Unknown Tool")
                            tool_input = getattr(action, "tool_input", {})
                            
                            if tool_name == "python_repl_ast" or tool_name == "enhanced_python_repl":  # ğŸ†•
                                query = tool_input.get("query", "")
                                entry = f"\n**ğŸ Python ì½”ë“œ ì‹¤í–‰ (Enhanced):**\n```python\n{query}\n```\n"
                            else:
                                entry = f"\n**ë„êµ¬ í˜¸ì¶œ: {tool_name}**\nì…ë ¥: {json.dumps(tool_input, indent=2, ensure_ascii=False)}\n"
                            accumulated_tool.append(entry)
                        
                        # ê´€ì°° ì²˜ë¦¬ - ğŸ†• DataFrame í˜¸í™˜ì„±ì„ ìœ„í•œ ì•ˆì „í•œ ì²´í¬  
                        if observation is not None and not (hasattr(observation, 'empty') and observation.empty):
                            obs_str = str(observation)
                            if len(obs_str) > 1000:
                                obs_str = obs_str[:1000] + "..."
                            entry = f"\n**ğŸ“Š ì‹¤í–‰ ê²°ê³¼:**\n```\n{obs_str}\n```\n"
                            accumulated_tool.append(entry)
                
                tool_placeholder.markdown("".join(accumulated_tool))
                time.sleep(0.01)
                return None
        
        # Handle different node types from AgentExecutor
        if node == "llm" and message_content:
            # LLM streaming chunks
            logging.debug(f"Processing LLM node with content: {message_content}")
            if hasattr(message_content, "content") and message_content.content:
                content = message_content.content
                if isinstance(content, str) and content.strip():
                    accumulated_text.append(content)
                    # Use markdown for better real-time display
                    text_placeholder.markdown("".join(accumulated_text))
                    # Force UI update
                    time.sleep(0.01)
                    logging.debug(f"Added LLM content to text: {content[:100]}...")
            return None
            
        elif node == "tool" and message_content:
            # Tool execution info
            logging.debug(f"Processing tool node with content: {message_content}")
            if isinstance(message_content, dict):
                if "tool" in message_content and "tool_input" in message_content:
                    # Tool start
                    tool_name = message_content["tool"]
                    tool_input = message_content["tool_input"]
                    entry = f"\n**ë„êµ¬ í˜¸ì¶œ: {tool_name}**\nì…ë ¥: {json.dumps(tool_input, indent=2, ensure_ascii=False)}\n"
                    accumulated_tool.append(entry)
                    tool_placeholder.markdown("".join(accumulated_tool))
                    time.sleep(0.01)
                    logging.debug(f"Added tool start info: {tool_name}")
                elif "tool" in message_content and "output" in message_content:
                    # Tool end
                    tool_name = message_content["tool"]
                    tool_output = message_content["output"]
                    entry = f"\n**ë„êµ¬ ê²°ê³¼: {tool_name}**\n{str(tool_output)[:1000]}...\n"
                    accumulated_tool.append(entry)
                    tool_placeholder.markdown("".join(accumulated_tool))
                    time.sleep(0.01)
                    logging.debug(f"Added tool result: {tool_name}")
            return None
            
        elif node == "agent" and message_content:
            # Final agent output
            logging.debug(f"Processing agent node with content: {message_content}")
            if isinstance(message_content, dict):
                output_text = message_content.get("output", "")
                if isinstance(output_text, str) and output_text.strip():
                    accumulated_text.append(output_text)
                    text_placeholder.markdown("".join(accumulated_text))
                    time.sleep(0.01)
                    logging.debug(f"Added agent output to text: {output_text[:100]}...")
            return None

        # Handle complete AIMessage (non-chunked)
        if isinstance(message_content, AIMessage):
            logging.debug("Processing complete AIMessage")
            content = message_content.content
            if isinstance(content, str):
                accumulated_text.append(content)
                text_placeholder.write("".join(accumulated_text))
                logging.debug(f"Added complete AIMessage content to text: {content[:100]}...")
            
            # Check for tool calls in additional_kwargs
            if hasattr(message_content, "additional_kwargs") and "tool_calls" in message_content.additional_kwargs and message_content.additional_kwargs["tool_calls"]:
                tool_calls = message_content.additional_kwargs["tool_calls"]
                logging.debug("Found tool_calls in AIMessage additional_kwargs")
                for tool_call in tool_calls:
                    tool_id = tool_call.get("id")
                    tool_name = tool_call.get("function", {}).get("name", "")
                    raw_arguments = tool_call.get("function", {}).get("arguments", None)
                    if isinstance(raw_arguments, str):
                        try:
                            args = json.loads(raw_arguments)
                        except:
                            args = {"raw_arguments": raw_arguments}
                    else:
                        args = raw_arguments
                    # Accumulate formatted tool call info
                    call_info = {"name": tool_name, "args": args, "id": tool_id, "type": "tool_call"}
                    # Display only raw_arguments when available
                    if raw_arguments is not None:
                        if isinstance(raw_arguments, str):
                            raw_display = raw_arguments
                        else:
                            raw_display = json.dumps(raw_arguments, indent=2, ensure_ascii=False)
                        if tool_name:
                            if tool_name=='execute_python' or tool_name == "enhanced_python_repl":  # ğŸ†•
                                entry = f"\n**ë„êµ¬ í˜¸ì¶œ: {tool_name}**\n\n{raw_display}\n"
                            else:
                                entry = f"\n**ë„êµ¬ í˜¸ì¶œ: {tool_name}**\n\n{raw_display}\n"
                        else:
                            entry = raw_display
                        accumulated_tool.append(entry)
                    # Store tool call for later processing (only once per call)
                    if tool_id and tool_name and tool_id not in seen_tool_call_ids:
                        seen_tool_call_ids.add(tool_id)
                        if "pending_tool_calls" not in st.session_state:
                            st.session_state.pending_tool_calls = []
                        st.session_state.pending_tool_calls.append({
                            "id": tool_id,
                            "name": tool_name,
                            "arguments": args
                        })
                        logging.debug(f"Added pending tool call: {tool_name} (id: {tool_id})")
                tool_placeholder.write("".join(accumulated_tool))
            
            return None

        # Handle AIMessageChunk
        if isinstance(message_content, AIMessageChunk):
            logging.debug("Processing AIMessageChunk")
            content = message_content.content
            
            # Content is list (e.g., Claude format)
            if isinstance(content, list) and len(content) > 0:
                logging.debug(f"AIMessageChunk content is list of length {len(content)}")
                message_chunk = content[0]
                if message_chunk.get("type") == "text":
                    text = message_chunk.get("text", "")
                    accumulated_text.append(text)
                    text_placeholder.write("".join(accumulated_text))
                    logging.debug(f"Added text chunk: {text[:100]}...")
                elif message_chunk.get("type") == "tool_use":
                    logging.debug("Processing tool_use chunk")
                    # Handle partial JSON fragments if available
                    if "partial_json" in message_chunk:
                        partial = message_chunk["partial_json"]
                        try:
                            pretty = json.dumps(json.loads(partial), indent=2, ensure_ascii=False)
                        except Exception:
                            pretty = partial
                        entry = f"\n```json\n{pretty}\n```\n"
                    else:
                        # Fallback to full tool_call_chunks
                        chunks = getattr(message_content, "tool_call_chunks", None)
                        if chunks:
                            chunk = chunks[0]
                            entry = f"\n```json\n{str(chunk)}\n```\n"
                        else:
                            entry = ""
                    accumulated_tool.append(entry)
                    tool_placeholder.write("".join(accumulated_tool))
                # Skip non-text, non-tool_use chunks
            # Check for OpenAI style tool calls
            elif (
                hasattr(message_content, "tool_calls")
                and message_content.tool_calls
                and len(message_content.tool_calls) > 0
            ):
                logging.debug("Found tool_calls attribute in AIMessageChunk")
                tool_call_info = message_content.tool_calls[0]
                tool_id = tool_call_info.get("id")
                tool_name = tool_call_info.get("function", {}).get("name", "")
                raw_arguments = tool_call_info.get("function", {}).get("arguments", None)
                if isinstance(raw_arguments, str):
                    try:
                        args = json.loads(raw_arguments)
                    except:
                        args = {"raw_arguments": raw_arguments}
                else:
                    args = raw_arguments
                call_info = {"name": tool_name, "args": args, "id": tool_id, "type": "tool_call"}
                # Display only raw_arguments when available
                if raw_arguments is not None:
                    if isinstance(raw_arguments, str):
                        raw_display = raw_arguments
                    else:
                        raw_display = json.dumps(raw_arguments, indent=2, ensure_ascii=False)
                    entry = raw_display
                    accumulated_tool.append(entry)
                # Store tool call for later processing
                if tool_id and tool_name and tool_id not in seen_tool_call_ids:
                    seen_tool_call_ids.add(tool_id)
                    if "pending_tool_calls" not in st.session_state:
                        st.session_state.pending_tool_calls = []
                    st.session_state.pending_tool_calls.append({
                        "id": tool_id,
                        "name": tool_name,
                        "arguments": args
                    })
                    logging.debug(f"Added pending tool call from chunk: {tool_name} (id: {tool_id})")
                tool_placeholder.write("".join(accumulated_tool))
            
            # Simple string content
            elif isinstance(content, str):
                accumulated_text.append(content)
                text_placeholder.write("".join(accumulated_text))
                logging.debug(f"Added string content: {content[:100]}...")
            
            # Invalid tool calls
            elif (
                hasattr(message_content, "invalid_tool_calls")
                and message_content.invalid_tool_calls
            ):
                logging.debug("Found invalid_tool_calls in AIMessageChunk")
                tool_call_info = message_content.invalid_tool_calls[0]
                tool_str = json.dumps(tool_call_info, indent=2)
                entry = tool_str.replace("\n", "")
                accumulated_tool.append(entry)
                tool_placeholder.write("".join(accumulated_tool))
                logging.debug(f"Added invalid tool call info: {tool_str[:100]}...")
            
            # Tool call chunks
            elif (
                hasattr(message_content, "tool_call_chunks")
                and message_content.tool_call_chunks
            ):
                logging.debug("Found tool_call_chunks in AIMessageChunk")
                tool_call_chunk = message_content.tool_call_chunks[0]
                tool_str = str(tool_call_chunk)
                entry = tool_str.replace("\n", "")
                accumulated_tool.append(entry)
                tool_placeholder.write("".join(accumulated_tool))
                logging.debug(f"Added tool call chunk: {tool_str[:100]}...")
            
            # Additional kwargs with tool calls
            elif (
                hasattr(message_content, "additional_kwargs")
                and "tool_calls" in message_content.additional_kwargs
                and message_content.additional_kwargs["tool_calls"]
            ):
                logging.debug("Found tool_calls in additional_kwargs")
                tool_calls = message_content.additional_kwargs["tool_calls"]
                for tool_call in tool_calls:
                    tool_id = tool_call.get("id")
                    tool_name = tool_call.get("function", {}).get("name", "")
                    raw_arguments = tool_call.get("function", {}).get("arguments", None)
                    if isinstance(raw_arguments, str):
                        try:
                            args = json.loads(raw_arguments)
                        except:
                            args = {"raw_arguments": raw_arguments}
                    else:
                        args = raw_arguments
                    call_info = {"name": tool_name, "args": args, "id": tool_id, "type": "tool_call"}
                    # Display only raw_arguments when available
                    if raw_arguments is not None:
                        if isinstance(raw_arguments, str):
                            raw_display = raw_arguments
                        else:
                            raw_display = json.dumps(raw_arguments, indent=2, ensure_ascii=False)
                        if tool_name:
                            entry = f"\n**ë„êµ¬ í˜¸ì¶œ: {tool_name}**\n{raw_display}\n"
                        else:
                            entry = raw_display.replace("\n", "")
                        accumulated_tool.append(entry)
                    if tool_id and tool_name and tool_id not in seen_tool_call_ids:
                        seen_tool_call_ids.add(tool_id)
                        if "pending_tool_calls" not in st.session_state:
                            st.session_state.pending_tool_calls = []
                        st.session_state.pending_tool_calls.append({
                            "id": tool_id,
                            "name": tool_name,
                            "arguments": args
                        })
                        logging.debug(f"Added pending tool call from kwargs: {tool_name} (id: {tool_id})")
                tool_placeholder.write("".join(accumulated_tool))
            
            else:
                logging.warning(f"Unhandled AIMessageChunk content format: {type(content)}")
        
        # Handle ToolMessage - this is the tool response
        elif isinstance(message_content, ToolMessage):
            logging.debug("Processing ToolMessage")
            tool_name = getattr(message_content, "name", "unknown_tool")
            tool_call_id = getattr(message_content, "tool_call_id", "unknown_id")
            content_str = str(message_content.content)
            
            logging.debug(f"ToolMessage received: name={tool_name}, tool_call_id={tool_call_id}")
            logging.debug(f"ToolMessage content: {content_str[:200]}...")
            
            # Store tool response for later processing
            if "tool_responses" not in st.session_state:
                st.session_state.tool_responses = {}
                
            st.session_state.tool_responses[tool_call_id] = {
                "name": tool_name,
                "content": content_str
            }
            
            # Find and remove from pending tools
            if "pending_tool_calls" in st.session_state:
                st.session_state.pending_tool_calls = [
                    call for call in st.session_state.pending_tool_calls 
                    if call.get("id") != tool_call_id
                ]
            
            # Add tool response JSON to the displayed tools
            try:
                response_data = json.loads(content_str)
                formatted = json.dumps(response_data, indent=2)
            except:
                formatted = content_str
            
            entry = f"\n\n**ë„êµ¬ í˜¸ì¶œ ê²°ê³¼: {tool_name}**\n\n{formatted}\n"
            
            accumulated_tool.append(entry)
            tool_placeholder.write("".join(accumulated_tool))
            logging.debug(f"Added tool response content: {formatted[:100]}...")
        
        # Fallback: handle original message format for backward compatibility
        elif message_content:
            logging.debug(f"Fallback processing - Message content type: {type(message_content)}")
            if isinstance(message_content, str):
                # actions/steps/output ë“± í‚¤ì›Œë“œë§Œ ë“¤ì–´ì˜¤ë©´ ë¬´ì‹œ
                if message_content.strip() in {"output", "actions", "steps", "intermediate_steps"}:
                    logging.debug(f"Ignored string '{message_content.strip()}' in streaming callback")
                    return None
                # ê·¸ ì™¸ì˜ strë§Œ ëˆ„ì (ì‹¤ì œ ë‹µë³€ í…ìŠ¤íŠ¸ê°€ strë¡œ ì˜¬ ë•Œë§Œ)
                accumulated_text.append(message_content)
                text_placeholder.markdown("".join(accumulated_text))
                logging.debug(f"Added string content to text: {message_content[:100]}...")
                return None
        else:
            logging.debug("No message content found")
        
        return None

    return callback_func, accumulated_text, accumulated_tool


def _handle_pandas_tool_display(tool, accumulated_tool, tool_placeholder):
    """pandas agent ë„êµ¬ í˜¸ì¶œ ì‹œ Streamlit í‘œì‹œ ì²˜ë¦¬"""
    tool_name = tool.get('tool', 'Unknown Tool')
    tool_input = tool.get('tool_input', {})
    
    if tool_name == "python_repl_ast" or tool_name == "enhanced_python_repl":
        query = tool_input.get("query", "")
        entry = f"\n**ğŸ Python ì½”ë“œ ì‹¤í–‰:**\n```python\n{query}\n```\n"
    else:
        entry = f"\n**ğŸ”§ ë„êµ¬ í˜¸ì¶œ: {tool_name}**\nì…ë ¥: {json.dumps(tool_input, indent=2, ensure_ascii=False)}\n"
    
    accumulated_tool.append(entry)
    tool_placeholder.markdown("".join(accumulated_tool))
    time.sleep(0.01)


def _handle_pandas_observation_display(observation, accumulated_tool, tool_placeholder):
    """pandas agent ê´€ì°° ê²°ê³¼ ì‹œ Streamlit í‘œì‹œ ì²˜ë¦¬"""
    obs = observation.get("observation")
    if obs is not None and not (hasattr(obs, 'empty') and obs.empty):
        obs_str = str(obs)
        if len(obs_str) > 1000:
            obs_str = obs_str[:1000] + "..."
        entry = f"\n**ğŸ“Š ì‹¤í–‰ ê²°ê³¼:**\n```\n{obs_str}\n```\n"
        accumulated_tool.append(entry)
        tool_placeholder.markdown("".join(accumulated_tool))
        time.sleep(0.01)


def _handle_pandas_result_display(result, accumulated_text, text_placeholder):
    """pandas agent ìµœì¢… ê²°ê³¼ ì‹œ Streamlit í‘œì‹œ ì²˜ë¦¬"""
    if isinstance(result, str) and result.strip():
        accumulated_text.append(result)
        text_placeholder.markdown("".join(accumulated_text))
        time.sleep(0.01)


# Handle tool execution for sync-compatible tools (ê°œì„ ëœ ë²„ì „)
async def execute_tool_sync_compatible(tool_call, tools):
    """Execute a sync-compatible tool and return its response - ê°œì„ ëœ ë²„ì „"""
    tool_id = tool_call.get("id")
    tool_name = tool_call.get("name")
    arguments = tool_call.get("arguments", {})
    
    logging.debug(f"Executing sync-compatible tool: {tool_name} (ID: {tool_id})")
    logging.debug(f"Arguments: {arguments}")
    
    # Find the matching tool
    matching_tool = None
    for tool in tools:
        if getattr(tool, "name", "") == tool_name:
            matching_tool = tool
            break
    
    if not matching_tool:
        error_msg = f"Tool {tool_name} not found"
        logging.error(error_msg)
        return {
            "tool_call_id": tool_id,
            "name": tool_name,
            "content": f"Error: {error_msg}"
        }
    
    try:
        # Execute the sync-compatible tool
        # ë™ê¸° í˜¸í™˜ ë„êµ¬ëŠ” .run() ë©”ì„œë“œë¥¼ ì‚¬ìš© (Tool í´ë˜ìŠ¤)
        if hasattr(matching_tool, 'run'):
            result = matching_tool.run(arguments)
        elif hasattr(matching_tool, 'invoke'):
            result = matching_tool.invoke(arguments)
        elif hasattr(matching_tool, 'func'):
            # Tool í´ë˜ìŠ¤ì˜ func ì†ì„±ì„ ì§ì ‘ í˜¸ì¶œ
            result = matching_tool.func(arguments)
        else:
            # Fallback to async invoke if available
            result = await matching_tool.ainvoke(arguments)
            
        logging.debug(f"Sync-compatible tool execution result: {str(result)[:200]}...")
        
        # Create response
        return {
            "tool_call_id": tool_id,
            "name": tool_name,
            "content": str(result)
        }
    except Exception as e:
        import traceback
        error_msg = f"Error executing sync-compatible tool {tool_name}: {str(e)}"
        error_trace = traceback.format_exc()
        logging.error(f"{error_msg}\n{error_trace}")
        return {
            "tool_call_id": tool_id,
            "name": tool_name,
            "content": f"Error: {error_msg}"
        }


# Handle tool execution
async def execute_tool(tool_call, tools):
    """Execute a tool and return its response"""
    tool_id = tool_call.get("id")
    tool_name = tool_call.get("name")
    arguments = tool_call.get("arguments", {})
    
    logging.debug(f"Executing tool: {tool_name} (ID: {tool_id})")
    logging.debug(f"Arguments: {arguments}")
    
    # Find the matching tool
    matching_tool = None
    for tool in tools:
        if getattr(tool, "name", "") == tool_name:
            matching_tool = tool
            break
    
    if not matching_tool:
        error_msg = f"Tool {tool_name} not found"
        logging.error(error_msg)
        return {
            "tool_call_id": tool_id,
            "name": tool_name,
            "content": f"Error: {error_msg}"
        }
    
    try:
        # Execute the tool with provided arguments
        result = await matching_tool.ainvoke(arguments)
        logging.debug(f"Tool execution result: {str(result)[:200]}...")
        
        # Create response
        return {
            "tool_call_id": tool_id,
            "name": tool_name,
            "content": str(result)
        }
    except Exception as e:
        import traceback
        error_msg = f"Error executing tool {tool_name}: {str(e)}"
        error_trace = traceback.format_exc()
        logging.error(f"{error_msg}\n{error_trace}")
        return {
            "tool_call_id": tool_id,
            "name": tool_name,
            "content": f"Error: {error_msg}"
        }


# Log function entry and exit
logging.debug('Entering function: process_query')
async def process_query(query, text_placeholder, tool_placeholder, timeout_seconds=60):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    pandas agentì™€ langgraph agent ëª¨ë‘ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. (ê°œì„ ëœ ë²„ì „)
    """
    try:
        if st.session_state.agent:
            logging.debug(f"Processing query: {query}")
            streaming_callback, accumulated_text_obj, accumulated_tool_obj = (
                get_streaming_callback(text_placeholder, tool_placeholder)
            )
            
            # Reset tool tracking for new query
            st.session_state.pending_tool_calls = []
            st.session_state.tool_responses = {}
            
            # ğŸ†• ìƒˆ ë©”ì‹œì§€ë¥¼ ìœ„í•´ ì‹œê°í™” ë°ì´í„° ì´ˆê¸°í™”
            st.session_state.current_message_visualizations = []
            
            try:
                logging.debug(f"Agent type: {type(st.session_state.agent)}")
                
                # Check if this is a pandas agent or regular LangGraph agent
                agent_type = st.session_state.get("agent_type", "unknown")
                
                if agent_type == "pandas":
                    # ğŸ”§ ìˆ˜ì •: pandas agentë¥¼ ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)
                    logging.debug("Processing pandas agent query using async wrapper")
                    
                    try:
                        # Clear the progress message
                        text_placeholder.markdown("")
                        
                        config = RunnableConfig(
                            recursion_limit=st.session_state.recursion_limit,
                            thread_id=st.session_state.thread_id,
                            configurable={
                                "callbacks": [
                                    lambda x: logging.debug(f"RunnableConfig callback: {str(x)[:100]}...")
                                ]
                            }
                        )
                        
                        logging.debug(f"Starting async pandas agent execution with timeout: {timeout_seconds}s")
                        
                        # ğŸ”§ ë¹„ë™ê¸° ë˜í¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° (ê°œì„ ëœ ë²„ì „)
                        final_output = ""
                        error_occurred = False
                        error_message = ""
                        step_count = 0
                        
                        # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸)
                        try:
                            async for step in st.session_state.agent.astream(
                                {"input": query}, 
                                config=config
                            ):
                                step_count += 1
                                logging.debug(f"Async pandas agent step {step_count}: {type(step)} - {list(step.keys()) if isinstance(step, dict) else step}")
                                
                                try:
                                    # Process the step using our callback
                                    streaming_callback({"node": "pandas_agent", "content": step})
                                    
                                    # Extract final output if available
                                    if isinstance(step, dict) and "output" in step:
                                        final_output = step["output"]
                                        logging.debug(f"Found final output: {str(final_output)[:100]}...")
                                        
                                    # ì‹¤ì‹œê°„ UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì‘ì€ ì§€ì—°
                                    await asyncio.sleep(0.01)
                                        
                                except Exception as step_error:
                                    error_occurred = True
                                    error_message = str(step_error)
                                    logging.warning(f"Error processing async pandas agent step: {error_message}")
                                    
                                    error_entry = f"\n**âš ï¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:**\n```\n{error_message}\n```\n"
                                    tool_placeholder.markdown(error_entry)
                                    continue
                        
                        except Exception as stream_error:
                            # ìŠ¤íŠ¸ë¦¬ë°ì´ ì‹¤íŒ¨í•œ ê²½ìš° ì§ì ‘ invoke ì‹œë„
                            logging.warning(f"Async streaming failed, trying direct invoke: {stream_error}")
                            try:
                                result = await st.session_state.agent.ainvoke(
                                    {"input": query}, 
                                    config=config
                                )
                                if isinstance(result, dict) and "output" in result:
                                    final_output = result["output"]
                                else:
                                    final_output = str(result)
                                    
                                # ì§ì ‘ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ì½œë°±ìœ¼ë¡œ ì²˜ë¦¬
                                streaming_callback({"node": "pandas_agent", "content": {"output": final_output}})
                                
                            except Exception as invoke_error:
                                error_occurred = True
                                error_message = f"Both streaming and direct invoke failed: {invoke_error}"
                                logging.error(error_message)
                        
                        # Handle tool calls if any were detected during streaming
                        if "pending_tool_calls" in st.session_state and st.session_state.pending_tool_calls:
                            logging.debug(f"Processing {len(st.session_state.pending_tool_calls)} pending tool calls")
                            
                            # Process each pending tool call
                            while st.session_state.pending_tool_calls:
                                tool_call = st.session_state.pending_tool_calls[0]
                                logging.debug(f"Processing tool call: {tool_call}")
                                
                                # ğŸ”§ ìˆ˜ì •: pandas agentìš© ë™ê¸° í˜¸í™˜ ë„êµ¬ë“¤ ì‚¬ìš© (ê°œì„ ëœ ë²„ì „)
                                available_tools = []
                                if st.session_state.mcp_client:
                                    # MCP í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë„êµ¬ ê°€ì ¸ì˜¤ê¸°
                                    mcp_tools = st.session_state.mcp_client.get_tools()
                                    # pandas agentì—ì„œëŠ” ë™ê¸° í˜¸í™˜ ë„êµ¬ë“¤ë¡œ ë³€í™˜
                                    for tool in mcp_tools:
                                        try:
                                            sync_tool = make_mcp_tool_sync_compatible(tool)
                                            available_tools.append(sync_tool)
                                        except Exception as e:
                                            logging.error(f"Failed to make tool {tool.name} sync compatible: {e}")
                                            continue
                                
                                # ë™ê¸° í˜¸í™˜ ë„êµ¬ë¡œ ì‹¤í–‰í•˜ë˜ ë¹„ë™ê¸° ì¸í„°í˜ì´ìŠ¤ ìœ ì§€
                                tool_result = await execute_tool_sync_compatible(tool_call, available_tools)
                                
                                # Create tool message
                                logging.debug(f"Tool result: {str(tool_result)[:200]}...")
                                tool_message = ToolMessage(
                                    content=tool_result["content"],
                                    name=tool_result["name"],
                                    tool_call_id=tool_result["tool_call_id"]
                                )
                                
                                # Display tool result
                                with tool_placeholder.expander("ğŸ”§ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼", expanded=True):
                                    st.write(f"**ë„êµ¬**: {tool_result['name']}\n\n**ê²°ê³¼**:\n```\n{tool_result['content'][:1000]}...\n```")
                                
                                # Remove processed tool call
                                st.session_state.pending_tool_calls = st.session_state.pending_tool_calls[1:]
                        
                        # ìµœì¢… ì¶œë ¥ ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)
                        if error_occurred and final_output:
                            final_output += f"\n\nâš ï¸ ì²˜ë¦¬ ì¤‘ ì¼ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì§€ë§Œ ê²°ê³¼ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤: {error_message}"
                            
                        elif error_occurred and not final_output:
                            final_output = f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_message}\n\nğŸ’¡ ë‹¤ìŒì„ ì‹œë„í•´ë³´ì„¸ìš”:\n- ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„\n- ë°ì´í„° í˜•íƒœë‚˜ ì»¬ëŸ¼ëª… í™•ì¸\n- ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ„ì–´ ì§ˆë¬¸"
                            
                        # Ensure final text is displayed
                        if final_output and not accumulated_text_obj:
                            accumulated_text_obj.append(final_output)
                            text_placeholder.markdown("".join(accumulated_text_obj))
                        
                        response = {"output": final_output}
                        logging.debug(f"Pandas agent processing completed. Steps: {step_count}, Final output length: {len(final_output)}")
                        
                    except Exception as e:
                        import traceback
                        error_msg = f"Async pandas agent ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                        error_trace = traceback.format_exc()
                        logging.error(f"{error_msg}\n{error_trace}")
                        
                        # ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ì •ë³´ ì œê³µ (ê°œì„ ëœ ë²„ì „)
                        user_friendly_error = f"""âŒ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

**ì˜¤ë¥˜ ë‚´ìš©:** {str(e)}

ğŸ’¡ **í•´ê²° ë°©ë²•:**
1. **ë°ì´í„° í™•ì¸**: `df.head()`, `df.info()`, `df.describe()` ë¡œ ë°ì´í„° ìƒíƒœ í™•ì¸
2. **ì»¬ëŸ¼ëª… í™•ì¸**: `df.columns.tolist()` ë¡œ ì •í™•í•œ ì»¬ëŸ¼ëª… í™•ì¸  
3. **ë‹¨ê³„ë³„ ì ‘ê·¼**: ë³µì¡í•œ ë¶„ì„ì„ ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ„ì–´ ìˆ˜í–‰
4. **êµ¬ì²´ì  ì§ˆë¬¸**: "íŠ¹ì • ì»¬ëŸ¼ì˜ í‰ê· ê°’ì€?" ê°™ì´ êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸

**ì¬ì‹œë„ ì˜ˆì‹œ:**
- "ë°ì´í„°ì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ë³´ì—¬ì¤˜"
- "ì²« 5í–‰ì„ ë³´ì—¬ì¤˜" 
- "ì»¬ëŸ¼ ì´ë¦„ì„ ì•Œë ¤ì¤˜"
"""
                        
                        # ìë™ìœ¼ë¡œ ê¸°ë³¸ ë°ì´í„° ì •ë³´ í™•ì¸ ì‹œë„
                        if st.session_state.dataframe is not None:
                            try:
                                df = st.session_state.dataframe
                                auto_info = f"""

ğŸ” **ìë™ ë°ì´í„° ì§„ë‹¨:**
- **ë°ì´í„° í¬ê¸°**: {df.shape[0]:,} í–‰ Ã— {df.shape[1]:,} ì—´
- **ì»¬ëŸ¼ëª…**: {', '.join(df.columns.tolist()[:10])}{'...' if len(df.columns) > 10 else ''}
- **ë°ì´í„° íƒ€ì…**: {df.dtypes.value_counts().to_dict()}
- **ê²°ì¸¡ê°’**: {df.isnull().sum().sum():,} ê°œ
"""
                                user_friendly_error += auto_info
                                    
                            except Exception as info_error:
                                logging.warning(f"Failed to get automatic data info: {info_error}")
                        
                        accumulated_text_obj.append(user_friendly_error)
                        text_placeholder.markdown("".join(accumulated_text_obj))
                        
                        response = {"output": user_friendly_error, "error": error_msg}
                        
                        
                else:
                    # ğŸ”§ langgraph agentëŠ” ê¸°ì¡´ ë°©ì‹ ìœ ì§€ (ì´ë¯¸ ë¹„ë™ê¸°) - 03_Agent.pyì™€ ë™ì¼
                    logging.debug("Processing LangGraph agent query")
                    config = RunnableConfig(
                        recursion_limit=st.session_state.recursion_limit,
                        thread_id=st.session_state.thread_id,
                        configurable={
                            "callbacks": [
                                lambda x: logging.debug(f"RunnableConfig callback: {str(x)[:100]}...")
                            ]
                        }
                    )
                    logging.debug(f"Starting agent execution with timeout: {timeout_seconds}s")
                    
                    # 03_Agent.pyì™€ ë™ì¼í•œ ë°©ì‹
                    agent_task = astream_graph(
                        st.session_state.agent,
                        {"messages": [HumanMessage(content=query)]},
                        callback=streaming_callback,
                        config=config,
                    )
                    
                    has_tool_calls = False
                    response = None
                    
                    try:
                        start_time = asyncio.get_event_loop().time()
                        remaining_time = timeout_seconds
                        
                        response = await asyncio.wait_for(
                            agent_task,
                            timeout=remaining_time
                        )
                        
                        logging.debug("Initial agent response received")
                        if "pending_tool_calls" in st.session_state and st.session_state.pending_tool_calls:
                            has_tool_calls = True
                            
                            while st.session_state.pending_tool_calls and remaining_time > 0:
                                logging.debug(f"Processing pending tool calls: {len(st.session_state.pending_tool_calls)}")
                                
                                tool_call = st.session_state.pending_tool_calls[0]
                                logging.debug(f"Processing tool call: {tool_call}")
                                
                                current_time = asyncio.get_event_loop().time()
                                elapsed = current_time - start_time
                                remaining_time = timeout_seconds - elapsed
                                
                                if remaining_time <= 0:
                                    logging.warning("Tool execution timeout")
                                    break
                                
                                # ğŸ”§ ìˆ˜ì •: MCP ë„êµ¬ë“¤ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (03_Agent.py ë°©ì‹)
                                available_tools = []
                                if st.session_state.mcp_client:
                                    # MCP í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë„êµ¬ ê°€ì ¸ì˜¤ê¸°
                                    mcp_tools = st.session_state.mcp_client.get_tools()
                                    available_tools.extend(mcp_tools)
                                    
                                tool_result = await asyncio.wait_for(
                                    execute_tool(tool_call, available_tools),
                                    timeout=remaining_time
                                )
                                
                                tool_message = ToolMessage(
                                    content=tool_result["content"],
                                    name=tool_result["name"],
                                    tool_call_id=tool_result["tool_call_id"]
                                )
                                
                                with tool_placeholder.expander("ğŸ”§ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼", expanded=True):
                                    st.write(f"**ë„êµ¬**: {tool_result['name']}\n\n**ê²°ê³¼**:\n```\n{tool_result['content'][:1000]}...\n```")
                                
                                current_time = asyncio.get_event_loop().time()
                                elapsed = current_time - start_time
                                remaining_time = timeout_seconds - elapsed
                                
                                if remaining_time <= 0:
                                    logging.warning("Agent continuation timeout")
                                    break
                                    
                                agent_continue_task = astream_graph(
                                    st.session_state.agent,
                                    {"messages": [tool_message]},
                                    callback=streaming_callback,
                                    config=config,
                                )
                                
                                response = await asyncio.wait_for(
                                    agent_continue_task,
                                    timeout=remaining_time
                                )
                                
                                st.session_state.pending_tool_calls = st.session_state.pending_tool_calls[1:]
                                
                                current_time = asyncio.get_event_loop().time()
                                elapsed = current_time - start_time
                                remaining_time = timeout_seconds - elapsed
                                
                                if not st.session_state.pending_tool_calls:
                                    logging.debug("No more pending tool calls")
                                    break
                                    
                    except asyncio.TimeoutError:
                        error_msg = f"â±ï¸ ìš”ì²­ ì‹œê°„ì´ {timeout_seconds}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
                        logging.error(f"Query timed out after {timeout_seconds} seconds")
                        return {"error": error_msg}, error_msg, ""
                        
                logging.debug("Query completed successfully")
                if hasattr(response, 'get'):
                    resp_content = response.get('content', 'No content')
                    logging.debug(f"Response content: {str(resp_content)[:100]}...")
                else:
                    logging.debug(f"Response type: {type(response)}")
                    
            except asyncio.TimeoutError:
                error_msg = f"â±ï¸ ìš”ì²­ ì‹œê°„ì´ {timeout_seconds}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
                logging.error(f"Query timed out after {timeout_seconds} seconds")
                return {"error": error_msg}, error_msg, ""
            except Exception as e:
                import traceback
                error_msg = f"ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                error_trace = traceback.format_exc()
                logging.error(f"{error_msg}\n{error_trace}")
                return {"error": error_msg}, error_msg, error_trace
                
            final_text = "".join(accumulated_text_obj)
            final_tool = "".join(accumulated_tool_obj)
            
            # If no streaming content was captured, try to extract from response
            if not final_text and response:
                if isinstance(response, dict):
                    if "output" in response:
                        final_text = str(response["output"])
                    elif "content" in response:
                        final_text = str(response["content"])
                    else:
                        final_text = str(response)
                else:
                    final_text = str(response)
                    
                # Update the placeholder with the final text
                text_placeholder.markdown(final_text)
            
            logging.debug(f"Final text length: {len(final_text)}")
            logging.debug(f"Final text: {final_text[:100]}...")
            logging.debug(f"Final tool content length: {len(final_tool)}")
            logging.debug(f"Final tool content: {final_tool[:100]}...")
            return response, final_text, final_tool
        else:
            logging.warning("Agent not initialized before query")
            return (
                {"error": "ğŸš« ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."},
                "ğŸš« ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "",
            )
    except Exception as e:
        import traceback
        error_msg = f"âŒ ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        error_trace = traceback.format_exc()
        logging.error(f"{error_msg}\n{error_trace}")
        return {"error": error_msg}, error_msg, error_trace


def load_selected_prompt():
    selected = st.session_state["prompt_selectbox"]
    prompts_dict = prompt_data.get("prompts", {})
    if selected in prompts_dict:
        st.session_state.selected_prompt_name = selected
        st.session_state.selected_prompt_text = prompts_dict[selected]["prompt"]
        st.session_state.prompt_loaded = True
        st.session_state["sidebar_edit_prompt_text"] = prompts_dict[selected]["prompt"]


def load_selected_tool():
    selected = st.session_state["tool_selectbox"]
    logging.debug(f"Selected tool: {selected}")
    selected_tool = next((t for t in tools_list if t["name"] == selected), None)
    if selected_tool:
        logging.debug(f"Loading tool configuration from: {selected_tool['path']}")
        try:
            with open(selected_tool["path"], encoding="utf-8") as f:
                st.session_state.tool_config = json.load(f)
            st.session_state.file_path = selected_tool["path"]
            st.session_state.loaded = True
            # Normalize pending MCP config: only keep valid connection fields
            raw_conf = st.session_state.tool_config.get("mcpServers", st.session_state.tool_config)
            pending_conf = {}
            for srv_name, srv_cfg in raw_conf.items():
                if "url" in srv_cfg:
                    # SSE connection
                    conf = {"transport": srv_cfg.get("transport", "sse"), "url": srv_cfg["url"]}
                    if "headers" in srv_cfg:
                        conf["headers"] = srv_cfg["headers"]
                    if "timeout" in srv_cfg:
                        conf["timeout"] = srv_cfg["timeout"]
                    if "sse_read_timeout" in srv_cfg:
                        conf["sse_read_timeout"] = srv_cfg["sse_read_timeout"]
                    if "session_kwargs" in srv_cfg:
                        conf["session_kwargs"] = srv_cfg["session_kwargs"]
                else:
                    # stdio connection
                    conf = {"transport": srv_cfg.get("transport", "stdio"), "command": srv_cfg["command"], "args": srv_cfg["args"]}
                    if "env" in srv_cfg:
                        conf["env"] = srv_cfg["env"]
                    if "cwd" in srv_cfg:
                        conf["cwd"] = srv_cfg["cwd"]
                    if "encoding" in srv_cfg:
                        conf["encoding"] = srv_cfg["encoding"]
                    if "encoding_error_handler" in srv_cfg:
                        conf["encoding_error_handler"] = srv_cfg["encoding_error_handler"]
                    if "session_kwargs" in srv_cfg:
                        conf["session_kwargs"] = srv_cfg["session_kwargs"]
                pending_conf[srv_name] = conf
            # Store direct mapping for initialization (initialize_session will unpack it)
            st.session_state.pending_mcp_config = pending_conf
            logging.debug("Tool configuration loaded successfully.")
        except Exception as e:
            logging.error(f"Error loading tool configuration: {str(e)}")


# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "session_initialized" not in st.session_state:
    logging.debug('Session state not initialized, setting default values')
    st.session_state.session_initialized = False  # ì„¸ì…˜ ì´ˆê¸°í™” ìƒíƒœ í”Œë˜ê·¸
    st.session_state.agent = None  # ReAct ì—ì´ì „íŠ¸ ê°ì²´ ì €ì¥ ê³µê°„
    st.session_state.agent_type = None  # ì—ì´ì „íŠ¸ íƒ€ì… (pandas ë˜ëŠ” langgraph)
    st.session_state.history = []  # ëŒ€í™” ê¸°ë¡ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    st.session_state.mcp_client = None  # MCP í´ë¼ì´ì–¸íŠ¸ ê°ì²´ ì €ì¥ ê³µê°„
    st.session_state.timeout_seconds = 180  # ì‘ë‹µ ìƒì„± ì œí•œ ì‹œê°„(ì´ˆ), ê¸°ë³¸ê°’ 120ì´ˆ
    st.session_state.selected_model = "gpt-4o"  # ê¸°ë³¸ ëª¨ë¸ ì„ íƒ
    st.session_state.recursion_limit = 100  # ì¬ê·€ í˜¸ì¶œ ì œí•œ, ê¸°ë³¸ê°’ 100
    st.session_state.selected_prompt_text = ""  # initialize selected prompt text
    st.session_state.temperature = 0.1  # ê¸°ë³¸ temperature ì„¤ì •
    st.session_state.dataframe = None          # ğŸ†• DataFrame ë³´ê´€ìš©
    st.session_state.pending_tool_calls = []  # ëŒ€ê¸° ì¤‘ì¸ ë„êµ¬ í˜¸ì¶œ ëª©ë¡
    st.session_state.tool_responses = {}  # ë„êµ¬ ì‘ë‹µ ì €ì¥ ê³µê°„
    st.session_state.current_message_visualizations = []  # ğŸ†• í˜„ì¬ ë©”ì‹œì§€ ì‹œê°í™” ì €ì¥

    # Load default system prompt if none selected
    try:
        with open("prompts/system_prompt.yaml", "r", encoding="utf-8") as f:
            sys_data = yaml.safe_load(f)
            default_prompt = sys_data.get("template", "")
            # store system prompt separately for tool usage and initialize selected prompt
            st.session_state.system_prompt_text = default_prompt
            st.session_state.selected_prompt_text = default_prompt
    except Exception as e:
        logging.warning(f"Failed to load system prompt: {e}")

    # Auto-load AI App settings from URL 'id' param
    query_params = st.query_params
    if "id" in query_params:
        app_id = query_params["id"]
        if not st.session_state.get("auto_loaded", False):
            try:
                with open("store/ai_app_store.json", "r", encoding="utf-8") as f:
                    ai_app_store = json.load(f)
                app_found = False
                for section in ai_app_store.get("AIAppStore", []):
                    if app_found:
                        continue
                    for app in section.get("apps", []):
                        url_parts = urlsplit(app.get("url", ""))
                        params = parse_qs(url_parts.query)
                        if params.get("id", [None])[0] == app_id:
                            st.session_state.selected_model = app.get("model", st.session_state.selected_model)
                            st.session_state.temperature = app.get("temperature", st.session_state.temperature)
                            prompt_text = app.get("prompt", "")
                            if prompt_text:
                                st.session_state.selected_prompt_text = prompt_text
                                st.session_state.sidebar_edit_prompt_text = prompt_text
                            tool_config = app.get("tools", {})
                            st.session_state.tool_config = tool_config
                            raw_conf = tool_config.get("mcpServers", tool_config)
                            pending_conf = {}
                            for srv_name, srv_cfg in raw_conf.items():
                                if "url" in srv_cfg:
                                    conf = {"transport": srv_cfg.get("transport", "sse"), "url": srv_cfg["url"]}
                                    for k in ["headers", "timeout", "sse_read_timeout", "session_kwargs"]:
                                        if k in srv_cfg:
                                            conf[k] = srv_cfg[k]
                                else:
                                    conf = {"transport": srv_cfg.get("transport", "stdio"), "command": srv_cfg.get("command"), "args": srv_cfg.get("args")}
                                    for k in ["env", "cwd", "encoding", "encoding_error_handler", "session_kwargs"]:
                                        if k in srv_cfg:
                                            conf[k] = srv_cfg[k]
                                pending_conf[srv_name] = conf
                            st.session_state.pending_mcp_config = pending_conf
                            st.session_state.auto_loaded = True
                            st.session_state.prompt_loaded = True
                            st.session_state.prompt_selectbox = ""
                            st.session_state.tool_selectbox = ""
                            st.session_state.loaded = True
                            st.session_state.app_title = app.get("title", "Universal Agent")
                            success = st.session_state.event_loop.run_until_complete(initialize_session(st.session_state.pending_mcp_config))
                            st.session_state.session_initialized = success
                            if success:
                                st.rerun()
                            app_found = True
                            break
                    if app_found:
                        break
            except Exception as e:
                st.error(f"Error loading AI App config: {e}")

if "thread_id" not in st.session_state:
    st.session_state.thread_id = random_uuid()

try:
    # Suppress async generator cleanup errors
    sys.set_asyncgen_hooks(finalizer=lambda agen: None)
except AttributeError as e:
    logging.error(f'AttributeError: {str(e)}')


# Load MCP config JSON paths for tools selection
MCP_CONFIG_DIR = "mcp-config"
os.makedirs(MCP_CONFIG_DIR, exist_ok=True)
json_paths = glob.glob(f"{MCP_CONFIG_DIR}/*.json")
if not json_paths and not os.path.exists(f"{MCP_CONFIG_DIR}/mcp_config.json"):
    default_config = {"mcpServers": {}}
    with open(f"{MCP_CONFIG_DIR}/mcp_config.json", "w", encoding="utf-8") as f:
        json.dump(default_config, f, indent=2, ensure_ascii=False)
    json_paths = [f"{MCP_CONFIG_DIR}/mcp_config.json"]

st.sidebar.markdown("##### ğŸ’¡ Storeì—ì„œ ì¥ë°”êµ¬ë‹ˆì— ë‹´ì€ Promptì™€ MCP Toolì„ ì¡°í•©í•˜ì—¬ ë‚˜ë§Œì˜ AI Agentë¥¼ ë§Œë“¤ì–´ ë³´ì„¸ìš”.")

# --- Prompt Store (í”„ë¡¬í”„íŠ¸ ì„ íƒ ë° ê´€ë¦¬) ---
# EMP_NO ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ê²½ë¡œ ì„¤ì •
PROMPT_CONFIG_DIR = "prompt-config"
logging.debug('Loading configuration from .env')
dotenv_path = find_dotenv()
load_dotenv(dotenv_path)
EMP_NO = os.getenv("EMP_NO", "default_emp_no")
EMP_NAME = os.getenv("EMP_NAME", "default_emp_name")
PROMPT_STORE_PATH = os.path.join(PROMPT_CONFIG_DIR, f"{EMP_NO}.json")

# í”„ë¡¬í”„íŠ¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì•ˆë‚´ ë©”ì‹œì§€ ì¶œë ¥
if not os.path.exists(PROMPT_STORE_PATH):
    st.sidebar.warning(f"{PROMPT_STORE_PATH} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Prompt Storeì—ì„œ ì¥ë°”êµ¬ë‹ˆ ì €ì¥ì„ ë¨¼ì € í•´ì£¼ì„¸ìš”.")
    prompt_data = {"prompts": {}}
else:
    with open(PROMPT_STORE_PATH, encoding="utf-8") as f:
        prompt_data = json.load(f)